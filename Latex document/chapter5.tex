%----------------------------------------------------------------------------
\chapter{Experimental results}\label{chap:Tests}
%----------------------------------------------------------------------------
In the following chapter the results of the system tests will be discussed. 

The speed and precision of the operation and the accuracy of the classification were tested using a set of diverse test videos.

The results are revised and analysed with various parameter settings.

A method to correct the errors caused by the miscalibration of the test videos is proposed as well.
Lastly, the proper operation speed is ensured by testing the performance of the application on the target hardware.

\section{Test videos}\label{sec:test_videos}
For testing purposes a set of test videos, that capture miscellaneous urban scenes, are used.
The videos were recorded at multiple places in Budapest.
To ensure the robustness of the application, they represent diverse weather, light and traffic conditions, including heavy shadows, night-time, dawn, sundown, and various traffic densities and road users.
The attributes of the test videos are presented in detail in table \ref{tab:test_videos}.

\begin{table}[!h]
	\begin{adjustbox}{center}
		\includetable{test_videos}
	\end{adjustbox}
	\caption{Characteristics of videos used for performance testing.}
	\label{tab:test_videos}
\end{table}

Besides the traffic and light conditions, in which the videos were recorded, table \ref{tab:test_videos}.~presents other important characteristics and specific settings, that determine the evaluation's quality and level of detail.

The "Lanes set" option specifies whether the centre of lanes are defined in the configuration files, as an initial setting for the video.
If it is, the parameter calculation is limited to these specified lane centre-regions on the Tripwire, that decreases the count of points processed, thus increases the operation speed.
However it affects the precision of classification as well, because the average length value is calculated from less samples (further details on the classification are in section \ref{sec:classification}.).

The point count of the video's ground truth sets the resolution of the precision measures.

Other, video-specific settings are also presented, when appropriate.
The higher sensitivity of MOG2 overreaction is required for shadow-dense streams, where light changes easily overwhelm the background-subtractor, and tend to increase the count of false negative detections.
A minimal blob size (minSize), smaller than the default setting is specified, whenever the possibility of smaller road-users (e.g. pedestrians, cyclists) is higher, to be able to detect them more precisely.
If the calibration is corrected, the precision and accuracy values of the video are higher, but this feature does not influence the overall effect of other settings changes. 
\FloatBarrier

%----------------------------------------------------------------------------
\section{Evaluation results}
%----------------------------------------------------------------------------
This section presents the results of the evaluation of the measures, that were described in detail in section \ref{chap:evaluation}.
The values were quantified using five different parameter-settings, each case specified to eliminate a specific, prevalent error.

\subsection{Versions of test settings}
The various settings used for testing are seen in table \ref{tab:settings_versions}.
Although the application uses several configuration parameters, in this section only the ones, that were modified between two evaluation session, are discussed.

As mentioned before (in section \ref{chap:calibration}.), for each video specific parameters are set, depending on the environment characteristics.
The particular values of these parameters were evaluated empirically for every video, and are not discussed in this work.

\begin{table}[!h]
	\begin{adjustbox}{center}
		\includetable{versions_settings}
	\end{adjustbox}
	\caption{The settings for each test session. If not defined, the minimal cutting size equals the minimal blob size.}
	\label{tab:settings_versions}
\end{table}

\noindent The parameters modified between test sessions are as follows.
\begin{enumerate}
	\item \textbf{Minimal size of blobs:} Although the system detects blobs of all sizes, only the ones exceeding the size limit are accepted as vehicles. If the minimal size decreases, the number of both false negatives and positives increases, since more fake and real vehicles are detected.
	
	\item \textbf{Lane centres defined or not:} The reason lane centres are specified is detailed in section \ref{sec:test_videos}. For some traffic scene (e.g. when there's no defined trajectory for vehicles) lane centres cannot be set (table \ref{tab:test_videos}), thus are not initially configured for some test videos. Consequently the evaluation results of certain videos are not affected by this parameter change. As described before, lane settings influence the result of classification, and the speed of the operation.
	
	\item \textbf{Minimal size for cutting blobs:} The parameter specifies the minimal size of split blobs. The minimal size is defined with a multiplier constant of the regular minimal size. Therefore split size differs for the videos, where a custom minimal size is set, including ErE5 or ErC1, as seen in table \ref{tab:test_videos}. 
	
	The method of blob splitting is presented in section \ref{sec:background-subtraction}., and is used to divide fused vehicles, that cross the Tripwire together. If the resulting blobs are of a smaller size than the limit, the splitting is cancelled, and the blob is stored as a whole. 
	
	The optimal minimal split size has been empirically estimated to be larger than the regular minimal blob size. This is due to the splitter algorithm's specific operation method, that cuts the blobs based on their shapes. If the parameter value equals the minimal size, some fused vehicles are missed, although, when set to a very low value, the number of false splitting, thus the count of false detections tend to increase.
\end{enumerate}

\subsection{Results}
\subsubsection{Initial settings}
The initial minimal size is \SI{100}{pixels}, resulting in several small noises detected as vehicles.
To correct these false alarms, firstly the vehicle size was increased.
The results are discussed in section \ref{sec:ver_2}.
To improve type recall, lane centres were removed, so that the whole length of the tripwire is analysed during size calculation. 
The effects of the change is explained in section \ref{sec:ver_3}.
To further improve the false positive rate, the split size was increased, so that the count of false blob divisions lessen (section \ref{sec:ver_4}).

\subsubsection{Minimal size of blobs increased}\label{sec:ver_2}
At the second testing stage, the minimal blob size was increased to \SI{200}{pixels}.
As seen on figure \ref{chart:false_positives}., the false positive rate decreased and the false negative rate increased (figure \ref{chart:false_negatives}) for every test video, although these changes have little effect on the overall precision and accuracy measures.

On videos, where a custom minimal size is added, the change has no effect.

\subsubsection{No lanes defined}\label{sec:ver_3}
The third version of settings were the elimination of lane centres, for which video they were defined.
Whether these points are defined, or not, do not affect the result of the detection (the value of error rates and precision measures, as seen on figures \ref{chart:accuracies}., \ref{chart:recalls}., \ref{chart:false_positives}., \ref{chart:false_negatives}.), but in almost every case, improves the classification results.

The drawback to the lane-less operation is the increased computational cost and time, as discussed in section \ref{sec:speed_tests}.

This settings change has no effect on videos with no lanes defined.

\subsubsection{Split size added}\label{sec:ver_4}
At the fourth test stage, the minimal split size is increased to a larger value than the regular minimal blob size.

As a result, as chart \ref{chart:recalls}.~shows, the recalls shifted only slightly, and most accuracies (chart \ref{chart:accuracies}.) either increased or stagnated correspondingly.
This is due to the increased number of false positives and also the decreased count of false negatives due to incorrectly uncut fusions (charts \ref{chart:false_positives}., \ref{chart:false_positives}). These two changes compensate each other's effects.

The performance measures somewhat improve with fine-tuning the size parameters (stage fifth), but the advancement is minor.

For videos with custom minimal sizes, the split size is adjusted accordingly, and set to a two times larger value.

\subsubsection{Conclusions}
Concluding the above analysis, the Traffic Sensor application is considered to be in a robust operating point to parameter changes, since the performance measures show only a slight shift when the settings are adjusted.

The system is robust to diverse traffic and light scenarios as well, as most test videos' recalls exceed \SI{80}{\%}, and accuracies are above \SI{70}{\%}. These values surpass the pre-defined \SI{70}{\%} limit.

It is important to mention that the videos' performance, that record standard urban road scenes with medium traffic density (e.g. GoldB2, GoldA1, FovamC\_D), are well over \SI{80}{\%} in recall, and \SI{70}{\%} in accuracy.
Therefore the Sensor's performance is predicted to be considerably accurate in general urban traffic scenes, furthermore, it operates over the required precision limit most of the time.

\subsubsection{Other error cases}
As the results charts show, some videos do not follow the common tendencies described above.
This is due to their particular traffic characteristics.

Test videos with extremely low accuracy count include SasadA1, ErC1, BorA1, BorA2 and FovamC\_N.
Each of the three videos include at least one of the three cases of a stopping tram, pedestrians as main road users, or heavy shadows.

\begin{figure}[!h]
	\centering
		\begin{subfigure}[b]{0.8\textwidth}
			\centering
			\begin{subfigure}[b]{\textwidth}
				\includegraphics[width=\textwidth]{ped_orig.png}
			\end{subfigure}
			\hspace{20mm}%
			\begin{subfigure}[b]{\textwidth}
				\includegraphics[width=\textwidth]{ped_res.png}
			\end{subfigure}
		\caption{Pedestrians detected as fused vehicles, and wrongly split into parts on the TIs of the video BorA2. \label{fig:error_peds}}
		\end{subfigure}
		
		\hfill
		
		\begin{subfigure}[b]{0.8\textwidth}
			\centering
			\begin{subfigure}[b]{\textwidth}
				\includegraphics[width=\textwidth]{tram_orig.png}
			\end{subfigure}
					 \hspace{6mm}%
			\begin{subfigure}[b]{\textwidth}
				\includegraphics[width=\textwidth]{tram_res.png}
			\end{subfigure}
			\caption{Slowing down tram on the TIs of video ErC1, that the stopping vehicle detection is unable to filter. \label{fig:error_tram}}
		\end{subfigure}
	
		\hfill
		
		\begin{subfigure}[b]{0.8\textwidth}
			\centering
			\begin{subfigure}[b]{\textwidth}
				\includegraphics[width=\textwidth]{shadow_orig.png}
			\end{subfigure}
								 \hspace{6mm}%
			\begin{subfigure}[b]{\textwidth}
				\includegraphics[width=\textwidth]{shadow_res.png}
			\end{subfigure}
		\caption{Heavy shadows causing an increased number of false alarms on the TIs of the video SasadA1. \label{fig:error_shadows}}
		\end{subfigure}
		
		\caption{Some particular traffic situation that challenge Traffic Sensor application. False detections are marked with a red square crossed over, and false negatives are marked with a blue dot. Correct detections are coloured.\label{fig:errors}}

\end{figure}

Although the application contains a mechanism to filter stopping vehicles, trams can overwhelm the system, and cause an increased amount of false positive detections, due to its moving parts as shown on figure \ref{fig:error_tram}.
Heavy shadows also raise the number of false alarms, because when they are moving with vehicles, the background-subtractor detects them (figure \ref{fig:error_shadows}).
Pedestrians tend to cause errors as well, as their blobs are often erroneously split, due to their characteristic shape that resembles two fused cars.
This case is only relevant when pedestrians are the primary target of detection, and are recorded from a closer distance, as seen on figure \ref{fig:error_peds}.

\begin{figure}[p]
	\centering
	\includechart{recalls}
	\caption{Recalls \label{chart:recalls}}
\end{figure}

\begin{figure}[p]
	\centering
	\includechart{accuracies}
	\caption{Accuracies \label{chart:accuracies}}
\end{figure}

\begin{figure}[p]
	\centering
	\includechart{typerecalls}
	\caption{Type recalls \label{chart:type_recalls}}
\end{figure}

\begin{figure}[p]
	\centering
	\includechart{falsePositives}
	\caption{False positives \label{chart:false_positives}}
\end{figure}

\begin{figure}[p]
	\centering
	\includechart{falseNegatives}
	\caption{False negatives \label{chart:false_negatives}}
\end{figure}

%----------------------------------------------------------------------------
\FloatBarrier
\section{Calibration correction}\label{chap:cal_corr}
%----------------------------------------------------------------------------
Due to perspective transform, vehicle lengths on frames decrease as moving away from the camera.
Since vehicle lengths are the basis of classification, to measure it precisely, perspective distortion needs to be eliminated.
The compensation of distortion is performed during the preprocessing phase of the Traffic Sensor operation, using a quadrangle-based compensation method.

The quadrangle, called the calibration rectangle, is manually defined by the user.
It is hand-drawn on a frame in the Project Configurator GUI (section \ref{chap:calibration}.) as part of the calibration process.
The position and the sides' lengths are estimated with the help of any occurring road lanes, or simply based on naked eye examination.
On this wise, due to the inaccuracy of human perception, the location and size of the calibration rectangle are significantly uncertain.
Empirical analysis shows that errors are actually present in many hand-calibrated videos.

To avoid manual calibration, some traffic surveillance systems automatize this process.
However in heterogeneous environment, it is a highly complex problem, and is not implemented in the current version of the Traffic Sensor system.
The solution is the automatized correction of existing settings that is revised by the user.

To evaluate the degree of miscalibration, lengths of the vehicles are examined as a function of distance from the camera.
After, during the finishing steps of the processing, the calibration rectangle is redefined for later use.
To eliminate false detections of miscalibration, the proposed corrections are revised and optionally affirmed by the user or the developer.

In the following section, the process of calibration, its correction method, and the experimental results of the improvement of calibration are detailed.

\subsection{The miscalibration problem}
\subsubsection{Perspective compensation method}
The calibration rectangle is constructed by defining its four edges from the upper left corner, to the lower left one in order.
The edges must be coplanar in the plane of the street, and correspond to those of a square's in the 3-dimensional (3D) world.
The square, due to the perspective transformation, converts to a general quadrilateral on the image plane.
On most videos, lane markings, that are parallel lines on the road, are used to find and define the required edges.
The length of the rectangle's sides are approximated and set by the user as well.

The four vertices are then projected back into a square, along with the entire pixel grid of the image using a transformation map to speed up the process.
Examples of the Frames, before and after the perspective compensation transformation, are seen in figure \ref{fig:perscomp}.
For defining the transform and constructing the map, universal OpenCV methods were used \cite{PersTrans, WrapPers}.

\begin{figure}[!t]
	\centering
		\begin{subfigure}[b]{0.36\textwidth}
			\includegraphics[width=\textwidth]{raw_frame_ibike.png}
			\caption{A Raw Frame from the ErE5 video sequence.}
		\end{subfigure}
		\quad
		\begin{subfigure}[b]{0.36\textwidth}
			\includegraphics[width=\textwidth]{original_frame_ibike.png}
			\caption{An Original Frame from the ErE5 video sequence.}
		\end{subfigure}
		\hfill
		\begin{subfigure}[b]{0.36\textwidth}
			\includegraphics[width=\textwidth]{raw_frame_fovam.png}
			\caption{A Raw Frame from the FovamC\_D video sequence.}
		\end{subfigure}
		\quad
		\begin{subfigure}[b]{0.36\textwidth}
			\includegraphics[width=\textwidth]{original_frame_fovam.png}
			\caption{An Original Frame from the FovamC\_D video sequence.}
		\end{subfigure}
		
		\caption{Frames before and after the perspective compensation. The calibration rectangle's edges are marked with their sequential numbers.\label{fig:perscomp}}
\end{figure}

In the output images of the transform -- the Original Frame-strip --  vehicle lengths are independent of the vehicle's position in the image. 

The calibration rectangle's side length is used to define a scaling factor between the 3D world and the image plane as well.
Using the scaling factor, all lengths are converted to a metric scale, facilitating correct classification.

\subsubsection{Miscalibrated streams}
If the calibration rectangle's edges are significantly displaced from those of a perfect coplanar square in the 3D world, the ensuing miscalibration considerably influences the accuracy of the classification results.
To detect whether this problem occurs, vehicle lengths are analysed along the tripwire.

\begin{figure}[!hp]
	\centering
	\begin{subfigure}[b]{\textwidth}
		\centering
		\begin{subfigure}[t]{0.45\textwidth}
		\includegraphics[width=\textwidth]{FovamC_raw_frame_uncalib.png}
		\end{subfigure}
	\quad
		\begin{subfigure}[t]{0.375\textwidth}
		\includegraphics[width=\textwidth]{FovamC_uncalibrated_example.png}
		\end{subfigure}
	\subcaption{Uncalibrated.\label{sfig:uncalib}}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{\textwidth}
		\centering
		\begin{subfigure}[t]{0.45\textwidth}
			\includegraphics[width=\textwidth]{FovamC_mcalib.png}
		\end{subfigure}
		\quad
		\begin{subfigure}[t]{0.375\textwidth}
			\includegraphics[width=\textwidth]{FovamC_miscalib.png}
		\end{subfigure}
		\subcaption{Miscalibrated. \label{sfig:mcalib}}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{\textwidth}
		\centering
		\begin{subfigure}[t]{0.45\textwidth}
			\includegraphics[width=\textwidth]{FovamC_ccalib_frame.png}
		\end{subfigure}
		\quad
		\begin{subfigure}[t]{0.375\textwidth}
			\includegraphics[width=\textwidth]{FovamC_ccalib.png}
		\end{subfigure}
		\subcaption{Correctly calibrated.}
	\end{subfigure}

	\caption{Frame setting and average vehicle lengths along the Tripwire on the FovamC\_D video sequence, when the video is uncalibrated, miscalibrated and correctly calibrated. The tripwire is drawn with red. Some vehicle lengths are marked to show the difference in distance from the camera. On the correctly calibrated frame the lengths are approximately the same. The light blue line indicates the estimated slope of the length values.\label{fig:calibration_versions}}
\end{figure}

An uncalibrated stream is characterized by the decrease of vehicle lengths as moving away from the camera, due to perspective distortion, as seen on figure \ref{sfig:uncalib}.
If the lengths still decrease despite that perspective compensation was performed, there is a high chance of inaccurate calibration (an example is seen in figure \ref{sfig:mcalib}.), when perspective distortion was not compensated well.
Nonetheless, some exceptions may occur, especially with a walking route and a bicycle track adjacent to the road, where the typical road users are of smaller size.

To accurately estimate the mean size of road users along a line, a considerably long video sequence is analysed to count enough vehicles for averaging.
The proper video length was empirically approximated to be at least 6000-frame long.

\subsection{The calibration correction method}
In the following section, the method, proposed to correct the erroneous calibration, is presented.

The three stages of the correction process are as follows:
\begin{enumerate}
\item At the first step, a line is fitted to the vehicle length data using linear regression.
\item Next, the average lengths at the two sides of the calibration rectangle are calculated from the parameters of the approximated line.
\item Last, a the ratio of these sizes are used to reshape the sides of the calibration rectangle.
\end{enumerate}

\noindent The calibration correction steps are detailed as follows:
\begin{enumerate}[align=parleft]
	\item  \textbf{Line estimation:} 
	Calibration correction is performed in single-run mode (section \ref{chap:operation_modes}), after the entire length of the video is processed.
	During the detection phase, in each point of the Tripwire, the average vehicle lengths are iteratively recalculated with the newly computed size values of the Size Timeline.
	The result of the calculation can then be visualized (as presented in section \ref{sec:data_representation}) as a curve of vehicle sizes as a function of position on the Tripwire.
	For quantifying the degree of the decrease in lengths, a line is fitted to the data set with simple least square approximation.
	The parameters of the fitted line ($fl(k)$) are stored.
	If the line is only slightly sloped, average lengths curve is approximately constant, thus there is no need for calibration correction. 
	
	\item  \textbf{Size ratio calculation:}
	First, the intersections of the Tripwire and the side lines of the calibration rectangle ($\boldsymbol{T}(K_{\text{CR}}), \boldsymbol{T}(k_{\text{CR}})$) are approximated.
	(Here, $k_{\text{CR}}$ and $K_{\text{CR}}$ denotes the intersection points' positions on the Tripwire in pixels.)
	
	Since the Traffic Sensor is placed on the side of the road, the typical frame scenery is customarily similar to the one seen on figure \ref{sfig:uncalib}.
	The calibration rectangle has two sides ($s_{\text{CR}}$ and $S_{\text{CR}}c$) roughly perpendicular to the Tripwire.
	Due to perspective transform, these perpendicular sides are subject to perspective distortion, thus differ in length, namely the top side of the rectangle is considerably shorter, as well as vehicle sizes at this distance.
	
	If the video is undercalibrated, the rectangle sides' difference is too small, so that after the perspective compensation, average vehicle sizes are still significantly smaller at the top side of the rectangle (an example is seen on figure \ref{sfig:mcalib}.).
	To diagnose this problem, the size values at the intersections are calculated using the fitted line:
	\begin{gather*}
		l = fl(k_{\text{CR}}) \approx \avgt\{\boldsymbol{TI_{\text{Size}}}(\boldsymbol{T}(k_{\text{CR}}))\} \\
		L = fl(K_{\text{CR}}) \approx \avgt\{\boldsymbol{TI_{\text{Size}}}(\boldsymbol{T}(K_{\text{CR}}))\},
	\end{gather*}
	
	with $fl(k_{\text{CR}}), fl(K_{\text{CR}})$ being the length values of the fitted line in the intersection points ($k_{\text{CR}}$ and $K_{\text{CR}}$) of the Tripwire.
	These length values correspond to the mean values of the length data stored in the proper rows of the Size Timeline.
	 
	After, the ratio of these two lengths is calculated, quantifying the degree of miscalibration, $r$:
	\begin{displaymath}
		r = \frac{L}{l}.
	\end{displaymath}
	
	The notations used above are illustrated in figure \ref{fig:ratio_calculation}.
	
	\item  \textbf{Rectangle correction:}
	After the ratio of the lengths are calculated, the longer side of the calibration rectangle, $S_{\text{CR}}$ is resized.
	Its length is multiplied with the ratio $r$ to produce the corrected length $S_{\text{CR,corr}}$.
	 To fit the new value, the side is expanded equally in both directions as seen on figure \ref{fig:resizing_cr}. 
	The numbering of edges stay consistent.
	After resizing, the length of the sides are re-evaluated as well.
\end{enumerate}

\begin{figure}[!h]
	\centering
	\includesvg[width=0.9\textwidth]{configuration_correction}
	\caption{Notation used in calibration correction, and the ratio calculation step of the process. The sizes at intersection of the Tripwire with the two intersecting calibration rectangle sides are evaluated, and the ratio of these length values are calculated. \label{fig:ratio_calculation}}
\end{figure}

\begin{figure}[!h]
	\centering
	\includesvg[width=0.4\textwidth,pretex=\relsize{1.5}]{resizing_of_cr}
	\caption{The resizing of the calibration rectangle. The larger side's length is multiplied with the ratio, $r$, and the side is expanded at each edges to fit the new value. All other sides are repositioned to fit the newly computed edges. \label{fig:resizing_cr}}
\end{figure}

\subsection{Calibration correction results}
The results of miscalibration are presented using four test videos (an overview of the test videos is found in section \ref{sec:test_videos}.), that were initially miscalibrated.

The quantitative evaluation results before and after the calibration correction are summarized in table \ref{tab:cal_corr_results}.
Three of four videos were tested with and without lanes defined.
Each video's type recall measures improved both with and without lanes defined. 
This is due to the the correct perspective compensation, resulting in correct lengths of vehicles.

Average vehicle sizes along the Tripwire for all four videos before and after correction are seen in figure \ref{fig:calibrations2}.
In each case, the length curves are more balanced after the correction.

\begin{table}[htbp]
	\centering
	\begin{adjustbox}{center}
	\includetable{cal_corr_table}
	\end{adjustbox}
	\caption{Table detailing the results of evaluation before and after calibration correction. The type recall values are emphasized with blue. Independently from lanes, the type recall of all videos improved after the correction. This is due to more realistic length measurements. The accuracy of some videos, including GoldA2 or GoldB1 (whithout lanes) have decreased due to the increase of false detections (increased false positive count). This is a result of expansion of blobs at the far end of the Tripwire, that, including the ones caused by noise, are not getting filtered. All measures are in \%.}
  	\label{tab:cal_corr_results}%
\end{table}%


\clearpage
\addtolength{\topmargin}{-.6in}
\begin{figure}[p]
	\thispagestyle{empty}
	\centering
	\thisfloatpagestyle{empty}
	\begin{subfigure}[t]{0.36\textwidth}
		\includegraphics[width=\textwidth]{calibrationCorrection_GoldA2_Not_OK.png}
		\caption{GoldA2, miscalibrated.}
	\end{subfigure}
	\quad
	\begin{subfigure}[t]{0.36\textwidth}
		\includegraphics[width=\textwidth]{calibrationCorrection_OK_GoldA2.png}
		\caption{GoldA2, corrected.}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.36\textwidth}
		\includegraphics[width=\textwidth]{calibrationCorrection_Fovam_not_OK.png}
		\caption{FovamC\_D, miscalibrated.}
	\end{subfigure}
	\quad
	\begin{subfigure}[t]{0.36\textwidth}
		\includegraphics[width=\textwidth]{calibrationCorrection_Fovam_OK.png}
		\caption{FovamC\_D, corrected.}
	\end{subfigure}
\hfill
\begin{subfigure}[t]{0.36\textwidth}
	\includegraphics[width=\textwidth]{calibrationCorrection_OK_SasA1_Med.png}
	\caption{SasA1, miscalibrated.}
\end{subfigure}
\quad
\begin{subfigure}[t]{0.36\textwidth}
	\includegraphics[width=\textwidth]{calibrationCorrection_not_OK_SasA1_Med.png}
	\caption{SasA1, corrected.}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.36\textwidth}
	\includegraphics[width=\textwidth]{calibrationCorrection_not_OK_GoldB1_Sun.png}
	\caption{GoldB1, miscalibrated.}
\end{subfigure}
\quad
\begin{subfigure}[t]{0.36\textwidth}
	\includegraphics[width=\textwidth]{calibrationCorrection_OK_GoldB1_Sun.png}
	\caption{GoldB1, corrected.}
\end{subfigure}
	\caption{Average vehicle lengths along the Tripwire calculated from a the same Frame-strip before and after calibration correction. After the correction, average sizes are distributed uniformly along the Tripwire.(The light blue line indicates the estimated slope of the length values.)\label{fig:calibrations2}}
\end{figure}

\clearpage
\addtolength{\topmargin}{+.6in}
%----------------------------------------------------------------------------
\section{Results of speed tests}\label{sec:speed_tests}
%----------------------------------------------------------------------------
Since the application is required to operate in real-time, it must meet strict speed criteria.
To not miss certain frames and consequently, vehicles, the software's frame request rate should exceed the camera's capture frequency most of the time.
Along these lines, the minimal recommended processing rate of frames on the target hardware is \SI{25}{frame/s}, considering that most cameras operate around this speed.

However, the optimal required operation speed of the Sensor is higher, \SI{30}{frame/s}, that even allows the integration of new functions, processing steps, or improving existing ones; especially, possible new features, that include highly resource-intensive algorithms, such as a shadow-detection processor (further discussion about shadow detection attempt is found in the work of Istv{\'a}n Vincze \cite{Vincze2016}), or a more sophisticated classifier.

The performance test were all conducted on the target hardware.

The results of speed-testing are seen in table \ref{tab:speed_testing}.

\begin{table}[htbp]
	\centering
	\begin{adjustbox}{center}
		\includetable{speed_results}
	\end{adjustbox}
	\caption{The speed performance of the Traffic Sensor software on the target hardware using test videos.}
	\label{tab:speed_testing}%
\end{table}%

As the results show, although the average operation speed depends on the traffic characteristics, in most situations, it exceeds the specified \SI{25}{frame/s} limit.
Exceptions are the GoldB2 and the SasA2 videos, that feature scenes with very heavy traffic and several stopping vehicles.

Considering the variance of the scenes, most videos still mainly perform over the minimum.
Although at certain times they are likely to fall below the speed floor, the performance rate is acceptable.
