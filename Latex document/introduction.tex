%----------------------------------------------------------------------------
\chapter*{Introduction}\addcontentsline{toc}{chapter}{Introduction}
%----------------------------------------------------------------------------
\section{Traffic load estimation}
%----------------------------------------------------------------------------
In recent years, there has been an increased scope for the automatic analysis of urban traffic activity\cite{Buch2011}.
This case is due to the increasing number of vehicles, the need for better understanding traffic dynamics for investment purposes, and also the accessibility of various sensors.
Formerly traffic estimation was performed by operators, by the means of on-the-spot observation or video analysis.
Automating these processes, the main concept of traffic analysis is to aid or fully avoid human operators.

The information obtained by traffic monitoring systems are widely used to regulate traffic flow and contribute to administrative decisions about transportation infrastructure development, maintenance and investments \cite{MagyarKozut}.
In addition to the detection and classification of road users, which is an elementary task, several other monitoring objectives can be supported. 
Typical traffic data such as flow, density, and average speed can be extracted.
Some systems also recognize various situations including traffic violations or accidents (e.g., cars, motorbikes, and pedestrians).
Other real-time applications notify users about congestions, roadworks and accidents, or control traffic lights in order to influence traffic flow intensity in one direction \cite{AzoSensor, Thiruverahan2015, Ghazal2016}.
It is also possible to estimate environmental measures such as greenhouse gases, pollutant emissions, and fuel consumption in real time using high-resolution video traffic data combined with instantaneous emission models\cite{Morris2012a}. 
In the long term surveillance systems will be integral parts of the smart city infrastructure, modelling, identifying and influencing these city's traffic dynamics\cite{enLight}.

\subsection{Traffic monitoring techniques}
Nowadays a variety of sensing modalities is available for on-road vehicle detection.

Inductive loop systems installed in the road are considered the most reliable traffic classification and detection method available, since larger metal objects can be accurately detected this way. 
On the other hand, the installation and maintenance is time-consuming and difficult, and slow or temporarily stopped vehicles cannot be detected\cite{Diamond, Zhang2016}.

Other active detection methods, including radar \cite{DeepBlue}, infrared\cite{Swarco, Hussain1995, Ghazal2016} and laser\cite{SICK, Gallego2009} sensors, are also popular owing to their robustness, although a complex machinery needs to be deployed.

Weight-based classification and speed detection is also possible using a piezoelectric sensor\cite{Te, Rivas2017}.

Video cameras have been installed for a long time for traffic monitoring purposes, because they provide a rich information source for human understanding with a relatively low cost of deployment\cite{Tian2011, Buch2011, VideoSurveillance, LaSemaforica}.
Nowadays cameras became even cheaper, smaller, and of higher quality than ever before, and vision-based surveillance is becoming the most popular form of vehicle detection.
In contradistinction to active sensors, cameras provide a wide field of view, allowing for detection across multiple lanes. 
Vision also integrates well with active sensor suites\cite{Garcia2012}.
The drawbacks to vision-based vehicle detection include sensitivity to light and weather conditions and increased computational cost\cite{Shivaraman2013}.

Multi-modal suites use sensor fusion to combine the data that results from the approaches outlined above\cite{Swarco}.

\subsubsection{Real-time vision sensors}
Although most vision sensor networks use a digital backend system where the captured data is transferred, and computation and storage takes place, since computing power has dramatically increased in the recent years, a new generation of visual surveillance systems has emerged.
These embedded sensors are capable of on-board high-level video processing with limited resources for computation, memory and power \cite{Bramberger2004}.
These platforms have the ability to reliably detect and track on-road vehicles in real time, since they utilize the new image-processing paradigms and advanced hardware: parallelization, multi-core processing and graphical processing units (GPUs)\cite{Sivaraman2013}.
Although the embedded platforms can provide sufficient computing performance, efficiently developing and porting software for these platforms is still a difficult and tedious task\cite{Bramberger2004}.
In this thesis an efficient traffic estimation method for embedded hardware is proposed, which can operate in real time for extended periods.
The implementation and realization details are discussed in the following chapters.

\subsection{Some results in video-based traffic monitoring}
In this section approaches in video-based on-road traffic monitoring are discussed, in particular with regard to techniques used in monocular vision.

Even though very diverse solutions has been suggested in the literature for vision-based surveillance problems, these methods are similar in the fact that they utilize the high information-content of the video stream, whilst the useless data are being identified and omitted for speed purposes.

Most vehicle counting methods are composed of three levels of perception depending upon each other: detection, tracking and behaviour understanding.
The two upper levels use information extracted from the lower ones, since vehicles are continuously detected across several frames for tracking purposes, and behaviour understanding relies strongly on the trajectory of the object.

At the first level, the goal is to tell vehicles apart from the background using distinctive features: appearance or motion.

On of the two main approaches is appearance-based selection, wherein basic image characteristics such as colour\cite{Chang2005}, edges\cite{Blanc2007}, symmetry\cite{Aytekin2010}, HOG\cite{TehraniNiknejad2012}, Haar-like\cite{Sivaraman2012}, Gabor\cite{Zhang2006}, SIFT\cite{Zhang2011} or SURF\cite{Lin2012} features are used to identify vehicles.
In recent years, there has been a transition from simpler image features like edges and symmetry to general and robust feature sets\cite{Sivaraman2012}.
These detection methods are both applicable for moving and stationary objects or cameras, but requires prior information about foreground object appearances.

These extracted features along with others are widely used for vehicle classification.
Most common distinctive attributes are heigh and length\cite{Huang2004}, linearity feature -- meaning the roughness of the vehicle contour\cite{Zhang2008} --, convex hull of the silhouette\cite{Buch2010} and distance between contour points\cite{Lou2005}.
Well-known descriptors like SIFT\cite{Zhang2011}, SURF\cite{Lin2012}, HOG\cite{Niknejad2012a} and a handful of others are also popular.
Automatic license plate recognition, which is also extensively used, relies on these features\cite{Luvizon2016}. 

Classifiers can be broadly split into two categories: the less popular discriminative classifiers, that learn the underlying distribution of a given class; and generative ones relying upon hard decision boundaries between classes\cite{Sivaraman2012}.
Discriminative methods include artificial neural network classifiers\cite{Xia2006}, support vector machines\cite{Cornelis2008}, k-nearest neighbour (kNN) classifier\cite{Morris2006}, and AdaBoost\cite{Khammari2005}.
Popular generative classifiers are hidden Markov models\cite{Jazayeri2011}, probabilistically weighted vote\cite{Lin2012}, and hidden random field\cite{Zhang2011}.

Motion-based vehicle detection has been less common than appearance-based methods.
The core idea is to separate moving foreground from motionless background.
This approach relies on background subtraction: the static scene is estimated with various models and moving object are separated accordingly.
Several filters have been proposed for background subtraction, including frame differencing\cite{Park2007}, averaging\cite{Kanhere2008}, graph cuts\cite{Woodford2009}, as well as more sophisticated background models, such as Single Gaussian\cite{Kumar2003}, mode estimation\cite{Zheng2006}, Kalman-filter\cite{Messelodi2005} or wavelets\cite{Gao2009}. 

One of the most common subtraction techniques is Gaussian Mixture Modelling(GMM)\cite{Niknejad2012, Wang2009,Zhang2016a}, wherein each pixel is temporally modelled as a mixture of two or more Gaussians and is updated online\cite{Stauffer1999,Stauffer2000}.
GMM is considered one of the best parametric background models that are widely used, owing to its robustness and effectiveness over different scenes\cite{Zhang2016}.
Although most subtraction techniques assume a fixed scene, GMM can deal with both stationery backgrounds and repetitive changes in light.
The implementation in \cite{Kaewtrakulpong2001} is available in the OpenCV library and is commonly used in research\cite{OpenCVMog2}.

One level up dynamic parameters are measured and the position of road-users is approximated using various tracking algorithms. 
The main purposes of following vehicles across frames is to estimate  motion, predict vehicle positions, and enforce temporal coherence to omit false positives in counting.
Tracking algorithms range from simplest -- like geometric constraints\cite{Rabe2007}, optical flow\cite{Bhaskar2015}, template matching\cite{Liu2007}, feature-based tracking\cite{Haselhoff2009} -- to most complex, including particle filtering\cite{Danescu2011}, Kalman-filter\cite{Bresson2015}, spatial–temporal Markov random field (S-T MRF)\cite{Zhu2005}, graph correspondence\cite{Lai2010} and event cones\cite{Andrienko2015} along with others. 

The highest level of semantic interpretation lies in characterizing the behaviour of vehicles on the road\cite{Sivaraman2013}.
An aggregate of spatio-temporal features is used to learn, model, classify, and predict the behaviour and goals of other vehicles on the road.
Certain studies categorize observed vehicle behaviour as normal or critical\cite{Cherng2009}, others identify specific maneuvers, like lane changing or turning\cite{Garcia2012}, some works try to forecast vehicle trajectory using trajectory-based predictors\cite{Hermes2009}, others focus on incident detection\cite{Kamijo2004}.
Maneuver-classification methods include hidden Markov model\cite{Sivaraman2011}, Bayesian networks\cite{Kasper2012} as well as Gaussian mixture modeling\cite{Wiest2012}.

Handling extreme weather or light conditions is always challenging for visual sensors, therefore a series of different solutions has been proposed to be able to process scenes with rain\cite{Yu2015,Barnum2010}, thick fog\cite{Zhou2014a,Tarel2009}, night-time\cite{Bi2009, Robert2009} or heavy shadows\cite{Kamkar2016, Miller2015}.

\subsubsection{Counting vehicles with time-spatial images}
Many traditional methods fail in real-time, embedded environments for traffic counting purposes, due to their high computational complexity.
Hence one of the challenges of real-time traffic monitoring is to decrease the aforementioned computation time by minimizing the information processed by filtering the image domain.

The selection of relevant information often relies on a priori knowledge of the scene, including direction of traffic flow and assumed trajectory of bypassing vehicles.
Many of these selection techniques restrict the calculation to an interested region in the traffic scene, e.g. virtual loops\cite{Tursun2013a, He2008} or detection regions\cite{Miller2015s, Engel2016} placed in the way of vehicles.

One of the most cost-effective, image-domain reducing techniques is based on virtual detection lines (VDLs) and time-spatial images (TSIs).
A virtual detection line is a one-pixel wide straight line placed perpendicular or parallel to the vehicle trajectory, whilst a time-spatial image can be considered itself as an image, where the vertical dimension corresponds to the VDL in the original frame, and the horizontal one corresponds to time in the original sequence.
A time-spatial image reflects the state of a fixed line changes with time, thus if a vehicle crosses or touches the VDL, it is shown on the TSI, becoming detectable\ref{fig:TSI}.
Such images can also be perceived as a sequence from a line-detector, or an information condenser. 

An approach similar to the aforementioned was first introduced in 2001 by Albiol et. al., and was proposed for counting people on video sequences using a "stack of lines" -- equivalent to a TSI\cite{Albiol2001}.
An approach with virtual line groups placed both perpendicular and parallel to the traffic flow direction was proposed later for counting\cite{Anan2006,Wu2007}.
A background subtraction method based on frame differencing was used in \cite{Anan2006, Wu2007}, wherein background estimation and detection took place solely on the VDLs.
This has the advantages of high operation speed and strong real-time ability, due to processing only a rather small portion of the image, however this method is unable to measure vehicle length and other distinctive features necessary for classification.
In 2008 Li et. al. proposed a time-spatial imagery based algorithm to analyse congestions on urban roads.
Li used perpendicular lines to detect and count vehicles in normal traffic flow, and parallel lines to identify a congestion scene\cite{Li2008}.
Yue evaluated traffic-flow parameters under urban road environment with the same method\cite{Yue2009}.
A similar technique was also introduced by Rashid et. al., wherein vehicles were detected and counted on a TSI, and feature-based classification was performed using attributes like width, area and rectangularity extracted from both the TSI and frames\cite{Rashid2010}.
Mithun et. al. proposes a kNN classification scheme using shape-based, shape-invariant, and texture-based features of the segmented regions corresponding to the vehicles on the TSIs\cite{Mithun2012a}.
Both Yue, Rashid and Mithn uses an appearance-based detection method, since TSIs are extracted from the original video sequence without background subtraction performed, and vehicles are identified using an edge detector.

In 2013 Yang et. al. discussed a new approach for time-spatial imagery extraction\cite{Yang2013a}.
Their motion-based method includes background subtraction and feature extraction (colour and edge invariants) on each frame before the TSI is created.
The resulting foreground- and feature-TSIs were used to detect vehicles, eliminate shadows and manage occlusions.
Yang also evaluated several traffic flow parameters, including velocity, flow, road occupancy and density by measurements carried out on the original frames. 
Vehicle length and region size was used to classify vehicles.

Kryjak et. al.'s concept is based on detecting the presence of vehicles by analysing only the local neighbourhood of a VDL.
Kryjak also implemented the method in real-time for an embedded hardware\cite{Kryjak2014}.

In a recent study by Kamkar et. al. generate TSIs to measure vehicle length and classify them using a pre-trained random forest classifier\cite{Kamkar2016}. 
Kamkar's approach analyses both the time-spatial domain, and corresponding regions on the original frames to extract classifying features.
Note that in this study TSIs are used solely for classification purposes, the detection is performed on the original images.

On of the latest studies on this topic is Zhang's, in which a foreground time-spatial image (FTSI) is proposed for counting vehicles in complex urban traffic scenes\cite{Zhang2016}.
In this study a self-adaptive sample consensus background model is created, and foreground is estimated only on the VDL.
Occlusion are filtered using the convexity of the objects of the FTSI.

As the above examples show, TSI approaches are often implemented for real-time monitoring due to its low computational cost.
It can be realized as a part of an intelligent transportation system owing to its ability to function under a wide range of traffic situations, like occlusion, dense traffic, slow or temporarily stopped vehicles; and also a variety of lighting conditions, including sunny weather, night, rain, twilight, etc.

The drawback to using a TSI-based method for detection purposes is that the analysis on the time-spatial imagery is one-dimensional, making it impossible to measure and extract features perpendicular to the detection line.
Therefore most systems utilize a combination of frame-based and TSI-based features for classification purposes and for making the counting more accurate, as seen in \cite{Kryjak2014, Yang2013a}. 
This approach can moderately slow down the processing, although opens a wide range of other possible usages, including occlusion detection and traffic flow parameter evaluation.
%----------------------------------------------------------------------------
\section{The SOLSUN project}
The intelligent transportation system detailed in this work is a part of a complex suite, the SOLSUN (Sustainable Outdoor Lighting \& Sensory Urban Networks) system.
The goal of the SOLSUN project is to deploy an intelligent sensor network installed within existing street lighting to monitor the urban environment.
The nodes of the network contain several sensors capable of monitoring emission, noise, weather and traffic.
The 

The project devoted to design the traffic sensor started in 2015 on ...

A dolgozatom témájaként bemutatott forgalomszámláló rendszer egy nagyobb egység – a
SOLSUN () projekt – keretében jött
létre. A projekt 2015 júniusában indult a BME Automatizálási és Alkalmazott Informatika Tanszékén
[32]. Célja egy intelligens, lámpatestekbe épített egységekb˝ol álló szenzorhálózat kialakítása.
A hálózat elemei számos szenzort egyesítenek – az érzékel˝ok adatokat gy˝ujtenek a leveg
˝oszennyezésr˝ol, zajról, és a forgalom jellemz˝oir˝ol, majd ezt megosztják a központtal egy kommunikációs
hálózaton keresztül. A cél nemcsak az adatgy˝ujtés, hanem az így kapott információk
ért˝o felhasználása az energiafogyasztás csökkentésére, a világítás költséghatékonyabbá tételére,
a leveg˝oszennyezés mérséklésére. Emellett az adatok a forgalom szervezéséhez, tervezéséhez is
hozzájárulnak.

The Sustainable Outdoor Lighting and Sensory Urban Network (SOLSUN) uses street lighting control systems to provide connectivity for ‘smart city’ applications. Using EnLight technology and the deployment of networked sensors to monitor the chosen urban environment, SOLSUN demonstrates how an intelligent city infrastructure can be created in a cost-effective and sustainable way.
The platform brings together a strong core of public, private and academic partners with the combined expertise to develop outcomes that can be exploited on a global scale.
Using street-lighting control systems to provide connectivity for ‘smart city’ applications is a cost-effective way to deploy the required network infrastructure.
Technology will be installed to reduce energy consumption at the same time as turning street lights into nodes on a scalable network that is also expandable for other applications. Sensors will capture data on air pollution, noise pollution and traffic density; information gathered will be used to address traffic congestion, another key contributor of Green House Gas (GHG) emissions in cities.
%----------------------------------------------------------------------------
\subsection{Project goals and constraints}
%precision, speed constraints, SW, HW
A series of efficient video processing algorithms and optimization techniques are proposed.
Surveillance is performed from a zenital position above the road.
An example of the camera view is depicted in figure \ref{fig:camera_position}.

%----------------------------------------------------------------------------
\section{Thesis goals and organization}
%----------------------------------------------------------------------------
In this work the operation and structure of the Solsun Traffic Sensor is detailed, in particular with regard to the core software architecture, the hardware environment, network organization and supplementary software elements.
The thesis focuses on the results of the performance tests and software experiments also, as well as further improvement propositions.

This thesis is structured as follows.
The \ref{chap:Concepsts}.~chapter introduces some basic terms and concepts essential for understanding the operation of the Traffic Sensor. 
The \ref{chap:Core software}.~chapter presents the structure of the core software side by side with the video processing steps.
In chapter \ref{chap:Environment}.~the supplementary software and hardware elements linking the system together are detailed.
The \ref{chap:Tests}.~chapter details the results of the hardware and software experiments, discusses prevalent error impacts, and proposes related methods to improve detection rate
