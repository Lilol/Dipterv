% Encoding: UTF-8

@Article{Buch2011,
  author   = {N. Buch and S. A. Velastin and J. Orwell},
  title    = {A Review of Computer Vision Techniques for the Analysis of Urban Traffic},
  journal  = {IEEE Transactions on Intelligent Transportation Systems},
  year     = {2011},
  volume   = {12},
  number   = {3},
  pages    = {920-939},
  month    = {Sept},
  issn     = {1524-9050},
  abstract = {Automatic video analysis from urban surveillance cameras is a fast-emerging field based on computer vision techniques. We present here a comprehensive review of the state-of-the-art computer vision for traffic video with a critical analysis and an outlook to future research directions. This field is of increasing relevance for intelligent transport systems (ITSs). The decreasing hardware cost and, therefore, the increasing deployment of cameras have opened a wide application field for video analytics. Several monitoring objectives such as congestion, traffic rule violation, and vehicle interaction can be targeted using cameras that were typically originally installed for human operators. Systems for the detection and classification of vehicles on highways have successfully been using classical visual surveillance techniques such as background estimation and motion tracking for some time. The urban domain is more challenging with respect to traffic density, lower camera angles that lead to a high degree of occlusion, and the variety of road users. Methods from object categorization and 3-D modeling have inspired more advanced techniques to tackle these challenges. There is no commonly used data set or benchmark challenge, which makes the direct comparison of the proposed algorithms difficult. In addition, evaluation under challenging weather conditions (e.g., rain, fog, and darkness) would be desirable but is rarely performed. Future work should be directed toward robust combined detectors and classifiers for all road users, with a focus on realistic conditions during evaluation.},
  doi      = {10.1109/TITS.2011.2119372},
  keywords = {computer vision;road traffic;solid modelling;traffic information systems;video surveillance;3D modeling;automatic video analysis;computer vision techniques;intelligent transport systems;object categorization;traffic rule violation;urban surveillance cameras;urban traffic analysis;vehicle interaction;Cameras;Computer vision;Pixel;Roads;Surveillance;Vehicles;Closed-circuit television (CCTV);intersection monitoring;road user counting;road users;traffic analysis;urban traffic;vehicle classification;vehicle detection;visual surveillance},
}

@Article{7442578,
  Title                    = {Pedestrian Density Analysis in Public Scenes With Spatiotemporal Tensor Features},
  Author                   = {K. Chen and J. K. Kämäräinen},
  Journal                  = {IEEE Transactions on Intelligent Transportation Systems},
  Year                     = {2016},

  Month                    = {July},
  Number                   = {7},
  Pages                    = {1968-1977},
  Volume                   = {17},

  Abstract                 = {Pedestrian density estimation is one of the key problems in intelligent transportation systems and has been widely applied to a number of applications in other fields of engineering. Counting-by-regression methods are more favorable for coping with such a problem owing to their robustness against interperson occlusion and relaxing the impractical requirement of a high video frame rate, compared to counting-by-detection and counting-by-clustering methods. However, imagery features in the existing counting-by-regression approaches are extracted from the whole region or spatially localized cells/pixels of each single video frame, which omits the unique motion patterns of the same pedestrians across the neighboring frames. In the light of this, this paper exploits a novel tensor-formed spatiotemporal feature representation and applies it in a multilinear regression learning framework, which can capture spatially distributed dynamic crowd patterns by discovering the latent multidimensional structural correlations of tensor features along both spatial (i.e., horizontal and vertical) and temporal dimensions. Extensive evaluation with the public UCSD and Shopping Mall benchmarks demonstrate superior performance of our approach to the state-of-the-art counting methods even when the surveillance data has a low frame rate.},
  Doi                      = {10.1109/TITS.2016.2516586},
  ISSN                     = {1524-9050},
  Keywords                 = {feature extraction;intelligent transportation systems;pedestrians;regression analysis;video signal processing;counting-by-regression approaches;counting-by-regression methods;distributed dynamic crowd patterns;imagery feature extraction;intelligent transportation systems;interperson occlusion;latent multidimensional structural correlations;multilinear regression learning framework;pedestrian density analysis;pedestrian density estimation;public UCSD;public scenes;shopping mall benchmarks;spatial dimension;spatiotemporal tensor features;temporal dimension;tensor-formed spatiotemporal feature representation;Correlation;Estimation;Feature extraction;Image edge detection;Robustness;Spatiotemporal phenomena;Tensile stress;Pedestrian density analysis;multilinear learning;regression;spatiotemporal features;tensor}
}

@Article{880967,
  Title                    = {An algorithm to estimate mean traffic speed using uncalibrated cameras},
  Author                   = {D. J. Dailey and F. W. Cathey and S. Pumrin},
  Journal                  = {IEEE Transactions on Intelligent Transportation Systems},
  Year                     = {2000},

  Month                    = {Jun},
  Number                   = {2},
  Pages                    = {98-107},
  Volume                   = {1},

  Abstract                 = {We present a novel approach to estimate traffic speed using a sequence of images from an uncalibrated camera. We assert that exact calibration is not necessary to estimate speed. Instead, we use: 1) geometric relationships inherently available in the image, 2) some common-sense assumptions that reduce the problem to a one-dimensional geometry, 3) frame differencing to isolate moving edges and track vehicles between frames, and 4) parameters from the distribution of vehicle lengths to estimate speed},
  Doi                      = {10.1109/6979.880967},
  ISSN                     = {1524-9050},
  Keywords                 = {automobiles;computational geometry;computer vision;edge detection;image sequences;motion estimation;optical tracking;road traffic;1D geometry;computer vision;frame differencing;image sequences;motion estimation;road vehicles;tracking;traffic speed;uncalibrated cameras;Calibration;Cameras;Geometry;Image edge detection;Layout;Roads;Traffic control;Transportation;Vehicle detection;Velocity measurement}
}

@Article{6494642,
  Title                    = {An Energy Minimization Approach to Automatic Traffic Camera Calibration},
  Author                   = {D. N. Dawson and S. T. Birchfield},
  Journal                  = {IEEE Transactions on Intelligent Transportation Systems},
  Year                     = {2013},

  Month                    = {Sept},
  Number                   = {3},
  Pages                    = {1095-1108},
  Volume                   = {14},

  Abstract                 = {We present a method for automatic calibration of traffic cameras. The problem is formulated as one of energy minimization in reduced road-parameter space, from which internal and external camera parameters are determined. Our approach combines bottom-up processing of a video to find a vanishing point, lines in the background, and a directed activity map, along with top-down processing to fit a road model to these detected features using Markov chain Monte Carlo (MCMC). Enhanced autocorrelation along the dashed lines is used in conjunction with a best-fit road model to find road-to-image parameters. To maximize both robustness to noise and flexibility (e.g., to handle cases in which the camera is looking straight down the road), a single-vanishing-point length-based approach (VWL, according to the taxonomy in the work of Kanhere and Birchfield) is used. On a large number of data sets exhibiting a wide variety of conditions (including distractions such as bridges and on/off-ramps), our approach performs well, achieving less than 10% error in measuring test lengths in all cases.},
  Doi                      = {10.1109/TITS.2013.2253553},
  ISSN                     = {1524-9050},
  Keywords                 = {Markov processes;Monte Carlo methods;cameras;computer vision;road traffic;traffic engineering computing;video signal processing;MCMC method;Markov chain Monte Carlo method;automatic traffic camera calibration;best-fit road model;camera parameters;dashed lines;directed activity map;energy minimization approach;road-parameter space;road-to-image parameters;single-vanishing-point length-based approach;Camera calibration;Markov chain Monte Carlo;computer vision;traffic monitoring}
}

@Article{5941065,
  Title                    = {Adaptive Background Modeling Integrated With Luminosity Sensors and Occlusion Processing for Reliable Vehicle Detection},
  Author                   = {A. Faro and D. Giordano and C. Spampinato},
  Journal                  = {IEEE Transactions on Intelligent Transportation Systems},
  Year                     = {2011},

  Month                    = {Dec},
  Number                   = {4},
  Pages                    = {1398-1412},
  Volume                   = {12},

  Abstract                 = {This paper presents a novel vehicle detection and tracking system with stationary camera that relies on a recursive background-modeling approach, i.e., the adaptive Poisson mixture model, which is integrated with a hardware module consisting of luminosity sensors. The luminosity information side channel allows the system to effectively handle rapid changes in illumination, which is typical of outdoor applications and bottleneck of the existing background pixel classification methods. A novel algorithm for detecting and removing partial and full occlusions among blobs is also proposed. Partial occlusions are detected by evaluating the ratio between the area of the vehicle and the area of the vehicle's convex hull and are suppressed by identifying a cutting line using curvature analysis. A predictive model of the shape and motion features of the vehicles over consecutive frames instead corrects the error of the previous levels when full occlusions or background-vehicle occlusions occur in the scene. Quantitative evaluation and comparisons on some real-world scenarios demonstrate that the proposed approach outperforms state-of-the-art methods in terms of both vehicle detection and processing time, particularly due to the robustness and the efficiency of the background-modeling algorithm.},
  Doi                      = {10.1109/TITS.2011.2159266},
  ISSN                     = {1524-9050},
  Keywords                 = {computer graphics;object detection;object tracking;stochastic processes;vehicles;adaptive background modeling;background pixel classification methods;luminosity information side channel;luminosity sensors;occlusion processing;reliable vehicle detection;vehicle tracking system;Adaptation model;Image motion analysis;Object detection;Object recognition;Object segmentation;Vehicle detection;Image motion analysis;object detection;object recognition;object segmentation}
}

@Article{7106548,
  Title                    = {Real-Time Multipedestrian Tracking in Traffic Scenes via an RGB-D-Based Layered Graph Model},
  Author                   = {S. Gao and Z. Han and C. Li and Q. Ye and J. Jiao},
  Journal                  = {IEEE Transactions on Intelligent Transportation Systems},
  Year                     = {2015},

  Month                    = {Oct},
  Number                   = {5},
  Pages                    = {2814-2825},
  Volume                   = {16},

  Abstract                 = {Multipedestrian tracking in traffic scenes is challenging due to cluttered backgrounds and serious occlusions. In this paper, we propose a layered graph model in image (RGB) and depth (D) domains for real-time robust multipedestrian tracking. The motivation is to investigate high-level constraints in RGB-D data association and to improve the optimization from the trajectory level to the layer level. To construct a layered graph, we define constraints in the depth domain so that pedestrian objects in the image domain are assigned to proper layers. We use pedestrian detection responses in the RGB domain as graph nodes, and we integrate 3-D motion, appearance, and depth features as graph edges. An online updating depth factor is defined to describe the depth relationships among the observations in and out of the layers, and the occlusion issue is processed with an analytical layer-level strategy. With a heuristic label switching algorithm, multiple pedestrian objects are optimally associated and tracked. Experiments and comparison on five public data sets show that our proposed approach significantly reduces pedestrian's ID switch and improves tracking accuracy in the cases of serious occlusions.},
  Doi                      = {10.1109/TITS.2015.2423709},
  ISSN                     = {1524-9050},
  Keywords                 = {feature extraction;graph theory;image colour analysis;image motion analysis;object detection;object tracking;optimisation;pedestrians;3D motion features;RGB domain;RGB-D data association;RGB-D-based layered graph model;analytical layer-level strategy;appearance features;cluttered backgrounds;depth domain constraints;depth features;depth relationships;graph edges;graph nodes;heuristic label switching algorithm;high-level constraints;image domain;layer level;online updating depth factor;optimization;pedestrian ID switch;pedestrian detection;pedestrian objects;real-time multipedestrian tracking;serious occlusions;tracking accuracy;traffic scenes;trajectory level;Cameras;Data models;Feature extraction;Linear programming;Real-time systems;Target tracking;Trajectory;Multi-pedestrian tracking;RGB-D data;layered graph model;occlusion}
}

@Article{5508422,
  Title                    = {Goal Evaluation of Segmentation Algorithms for Traffic Sign Recognition},
  Author                   = {H. Gomez-Moreno and S. Maldonado-Bascon and P. Gil-Jimenez and S. Lafuente-Arroyo},
  Journal                  = {IEEE Transactions on Intelligent Transportation Systems},
  Year                     = {2010},

  Month                    = {Dec},
  Number                   = {4},
  Pages                    = {917-930},
  Volume                   = {11},

  Abstract                 = {This paper presents a quantitative comparison of several segmentation methods (including new ones) that have successfully been used in traffic sign recognition. The methods presented can be classified into color-space thresholding, edge detection, and chromatic/achromatic decomposition. Our support vector machine (SVM) segmentation method and speed enhancement using a lookup table (LUT) have also been tested. The best algorithm will be the one that yields the best global results throughout the whole recognition process, which comprises three stages: 1) segmentation; 2) detection; and 3) recognition. Thus, an evaluation method, which consists of applying the entire recognition system to a set of images with at least one traffic sign, is attempted while changing the segmentation method used. This way, it is possible to observe modifications in performance due to the kind of segmentation used. The results lead us to conclude that the best methods are those that are normalized with respect to illumination, such as RGB or Ohta Normalized, and there is no improvement in the use of Hue Saturation Intensity (HSI)-like spaces. In addition, an LUT with a reduction in the less-significant bits, such as that proposed here, improves speed while maintaining quality. SVMs used in color segmentation give good results, but some improvements are needed when applied to achromatic colors.},
  Doi                      = {10.1109/TITS.2010.2054084},
  ISSN                     = {1524-9050},
  Keywords                 = {edge detection;image classification;image colour analysis;image enhancement;image segmentation;road traffic;support vector machines;table lookup;LUT;SVM segmentation method;achromatic colors;chromatic-achromatic decomposition;color segmentation;color-space thresholding;edge detection;goal evaluation method;hue saturation intensity;image recognition;lookup table;speed enhancement;support vector machine;traffic sign recognition;Cameras;Image edge detection;Image recognition;Image segmentation;Pixel;Support vector machine classification;Support vector machines;Table lookup;Testing;Vehicles;Detection;recognition;segmentation;support vector machines (SVMs);traffic sign}
}

@Article{7017578,
  Title                    = {Model-Based Methodology for Validation of Traffic Flow Detectors by Minimizing Human Bias in Video Data Processing},
  Author                   = {P. Kachroo and N. Shlayan and A. Paz and S. Sastry and S. K. Patel},
  Journal                  = {IEEE Transactions on Intelligent Transportation Systems},
  Year                     = {2015},

  Month                    = {Aug},
  Number                   = {4},
  Pages                    = {1851-1860},
  Volume                   = {16},

  Abstract                 = {This paper provides a model-based method for analysis and hypothesis testing for paired data where one source of data has to be validated against another source of data that contains subjective and dynamic errors. This study deals with human-observed flow counts collected from traffic videos of freeway cameras. The available videos are mainly used for the purpose of manual observation by transportation personnel in case of emergency. This amounts to a varying inconsistency of the quality of the videos, which presents an additional challenge when analyzing the data. Video processing cannot be performed due to the mentioned issues with regard to the video quality. The processing has to be manually performed by humans who unfortunately have an inherent bias. If the video data have to be used for validating flow detector sensors, then a technique that performs validation with subjective and dynamic erroneous data as a result of the human bias is needed. This paper presents a methodology to deal with this issue. It is based on statistical testing with heteroscedasticity, which is demonstrated through a case study using data from traffic flow detectors and traffic cameras installed on highways in the Southern Nevada Region. A model for the relationship between the video ratings and the distribution of the human errors is developed taking into consideration the human bias. A method for identification of faulty detectors is also demonstrated based on the developed technique.},
  Doi                      = {10.1109/TITS.2014.2377552},
  ISSN                     = {1524-9050},
  Keywords                 = {statistical testing;traffic engineering computing;video signal processing;flow detector sensors;heteroscedasticity;human bias minimization;human error distribution;human-observed flow counts;model-based methodology;statistical testing;traffic flow detector validation;transportation personnel;video data processing;video quality;video ratings;Detectors;Fault detection;Manuals;Nickel;Random variables;Vehicles;Flow detectors;Welch's t-test;Welch's t-test;human observation bias;traffic videos;validation}
}

@Article{4405641,
  Title                    = {Real-Time Incremental Segmentation and Tracking of Vehicles at Low Camera Angles Using Stable Features},
  Author                   = {N. K. Kanhere and S. T. Birchfield},
  Journal                  = {IEEE Transactions on Intelligent Transportation Systems},
  Year                     = {2008},

  Month                    = {March},
  Number                   = {1},
  Pages                    = {148-160},
  Volume                   = {9},

  Abstract                 = {We present a method for segmenting and tracking vehicles on highways using a camera that is relatively low to the ground. At such low angles, 3-D perspective effects cause significant changes in appearance over time, as well as severe occlusions by vehicles in neighboring lanes. Traditional approaches to occlusion reasoning assume that the vehicles initially appear well separated in the image; however, in our sequences, it is not uncommon for vehicles to enter the scene partially occluded and remain so throughout. By utilizing a 3-D perspective mapping from the scene to the image, along with a plumb line projection, we are able to distinguish a subset of features whose 3-D coordinates can be accurately estimated. These features are then grouped to yield the number and locations of the vehicles, and standard feature tracking is used to maintain the locations of the vehicles over time. Additional features are then assigned to these groups and used to classify vehicles as cars or trucks. Our technique uses a single grayscale camera beside the road, incrementally processes image frames, works in real time, and produces vehicle counts with over 90% accuracy on challenging sequences.},
  Doi                      = {10.1109/TITS.2007.911357},
  ISSN                     = {1524-9050},
  Keywords                 = {automated highways;feature extraction;hidden feature removal;image classification;image segmentation;image sequences;object detection;road vehicles;tracking;feature tracking;grayscale camera;image sequences;low camera angles;occlusion reasoning;perspective mapping;real-time incremental segmentation;using stable features;vehicle classification;vehicle tracking;Feature tracking;occlusion;perspective projection;spillover;vehicle tracking}
}

@Article{7782816,
  Title                    = {Traffic Velocity Estimation From Vehicle Count Sequences},
  Author                   = {T. Katsuki and T. Morimura and M. Inoue},
  Journal                  = {IEEE Transactions on Intelligent Transportation Systems},
  Year                     = {2016},
  Number                   = {99},
  Pages                    = {1-13},
  Volume                   = {PP},

  Abstract                 = {Traffic velocity is a fundamental metric for inferring traffic conditions. This paper proposes a new velocity estimation approach from temporal sequences of vehicle count that does not require tracking any vehicles or using any labeled data. It is useful for measuring traffic velocities with low quality and inexpensive sensors such as web cameras in general use. We formalize the task as a density estimation problem by introducing a new model for temporal sequences of vehicle counts wherein the correlation between the sequences is directly related to the traffic velocity. We also derive a sampling-based algorithm for the density estimation. We show the effectiveness of our method on artificial and real-world data sets.},
  Doi                      = {10.1109/TITS.2016.2628384},
  ISSN                     = {1524-9050},
  Keywords                 = {Cameras;Correlation;Estimation;Roads;Sensors;Vehicles;Bayes procedures;Intelligent transportation systems;unsupervised learning;velocity measurement.}
}

@Article{6353219,
  Title                    = {Efficient Traffic State Estimation for Large-Scale Urban Road Networks},
  Author                   = {Q. J. Kong and Q. Zhao and C. Wei and Y. Liu},
  Journal                  = {IEEE Transactions on Intelligent Transportation Systems},
  Year                     = {2013},

  Month                    = {March},
  Number                   = {1},
  Pages                    = {398-407},
  Volume                   = {14},

  Abstract                 = {This paper presents a systematic solution to efficiently estimate the traffic state of large-scale urban road networks. We first propose the new approach to construct the exact GIS-T digital map. The exact digital map can lay the solid foundation for the traffic state estimation with the data from Global Positioning System (GPS) probe vehicles. Then, we present the following two effective methods based on GPS probe vehicles for the traffic state estimation: (1) the curve-fitting-based method and (2) the vehicle-tracking-based method. Finally, we test the proposed solution with a large number of real data from GPS probe vehicles and the standard digital map of Shanghai, China. In the experiments, data from thousands of GPS-equipped taxies were taken as the probe vehicles. The estimation accuracy and operation speed of the two different methods were systematically measured and compared. In addition, the coverages of the GPS sampling points were also investigated for the large-scale urban road network in the spatial and temporal domains. For the accuracy experiment, the ground truth was obtained by repeating the videos that were recorded on 24 road sections in downtown Shanghai. The experimental results illustrate that the proposed methods are effective and efficient in monitoring the traffic state of large-scale urban road networks.},
  Doi                      = {10.1109/TITS.2012.2218237},
  ISSN                     = {1524-9050},
  Keywords                 = {Global Positioning System;curve fitting;geographic information systems;object tracking;road traffic;road vehicles;sampling methods;video recording;China;GPS probe vehicles;GPS sampling point coverages;GPS-equipped taxies;Global Positioning System probe vehicles;Shanghai;curve-fitting-based method;estimation accuracy;exact GIS-T digital map;large-scale urban road network traffic state estimation;operation speed;road sections;spatial domains;standard digital map;temporal domains;vehicle-tracking-based method;video recording;Accuracy;Global Positioning System;Probes;Roads;State estimation;Vehicles;Coordinate transformation;GIS-T digital map;Global Positioning System (GPS) probe vehicle;large-scale road network;traffic state estimation}
}

@Article{6731512,
  Title                    = {Vision-Only Localization},
  Author                   = {H. Lategahn and C. Stiller},
  Journal                  = {IEEE Transactions on Intelligent Transportation Systems},
  Year                     = {2014},

  Month                    = {June},
  Number                   = {3},
  Pages                    = {1246-1257},
  Volume                   = {15},

  Abstract                 = {Autonomous and intelligent vehicles will undoubtedly depend on an accurate ego localization solution. Global navigation satellite systems suffer from multipath propagation rendering this solution insufficient. Herein, we present a real-time system for six-degrees-of-freedom ego localization that uses only a single monocular camera. The camera image is harnessed to yield an ego pose relative to a previously computed visual map. We describe a process to automatically extract the ingredients of this map from stereoscopic image sequences. These include a mapping trajectory relative to the first pose, global scene signatures and local landmark descriptors. The localization algorithm then consists of a topological localization step that completely obviates the need for any global positioning sensors such as GNSS. A metric refinement step that recovers an accurate metric pose is subsequently applied. Metric localization recovers the ego pose in a factor graph optimization process based on local landmarks. We demonstrate centimeter-level accuracy by a set of experiments in an urban environment. To this end, two localization estimates are computed for two independent cameras mounted on the same vehicle. These two independent trajectories are thereafter compared for consistency. Finally, we present qualitative experiments of an augmented reality (AR) system that depends on the aforementioned localization solution. Several screen shots of the AR system are shown confirming centimeter-level accuracy and subdegree angular precision.},
  Doi                      = {10.1109/TITS.2014.2298492},
  ISSN                     = {1524-9050},
  Keywords                 = {Global Positioning System;SLAM (robots);augmented reality;feature extraction;graph theory;image sequences;intelligent transportation systems;object detection;optimisation;pose estimation;real-time systems;rendering (computer graphics);road vehicles;robot vision;sensor placement;stereo image processing;video cameras;AR system;GNSS;accurate metric pose;augmented reality system;automatic map ingredient extraction;autonomous vehicles;centimeter level accuracy;degrees-of-freedom ego localization;ego pose recovery;factor graph optimization process;global navigation satellite systems;global positioning sensor;global scene signatures;intelligent vehicles;local landmark descriptors;localization algorithm;mapping trajectory;metric localization;metric refinement;monocular camera;multipath propagation rendering;real-time system;stereoscopic image sequences;topological localization;urban environment;vision-only localization;Accuracy;Cameras;Global Positioning System;Trajectory;Vectors;Vehicles;Visualization;Bundle adjustment;camera;global positioning system (GPS);landmark;localization;nonlinear least squares (NLS);simultaneous localization and mapping (SLAM)}
}

@Article{6480875,
  Title                    = {Vehicle Detection Based on the and--or Graph for Congested Traffic Conditions},
  Author                   = {Y. Li and B. Li and B. Tian and Q. Yao},
  Journal                  = {IEEE Transactions on Intelligent Transportation Systems},
  Year                     = {2013},

  Month                    = {June},
  Number                   = {2},
  Pages                    = {984-993},
  Volume                   = {14},

  Abstract                 = {In urban traffic video monitoring systems, traffic congestion is a common scene that causes vehicle occlusion and is a challenge for current vehicle detection methods. To solve the occlusion problem in congested traffic conditions, we have proposed an effective vehicle detection approach based on an and -or graph (AOG) in this paper. Our method includes three steps: constructing an AOG for representing vehicle objects in the congested traffic condition; training parameters in the AOG; and, finally, detecting vehicles using bottom-up inference. In AOG construction, sophisticated vehicle feature selection avoids using the easily occluded vehicle components but takes highly visible components into account. The vehicles are well represented by these selected vehicle features in the presence of a congested condition with serious vehicle occlusion. Furthermore, a hierarchical decomposition of the vehicle representation is proposed during AOG construction to further reduce the impact of vehicle occlusion. After AOG construction, all parameters in the AOG are manually learned from the training images or set and further applied to the bottom-up vehicle inference. There are two innovations of our method, i.e., the usage of the AOG in vehicle detection under congested traffic conditions and the special vehicle feature selection for vehicle representation. To fully test our method, we have done a quantitative experiment under a variety of traffic conditions, a contrast experiment, and several experiments on congested conditions. The experimental results illustrate that our method can effectively deal with various vehicle poses, vehicle shapes, and time-of-day and weather conditions. In particular, our approach performs well in congested traffic conditions with serious vehicle occlusion.},
  Doi                      = {10.1109/TITS.2013.2250501},
  ISSN                     = {1524-9050},
  Keywords                 = {feature extraction;graph theory;image representation;inference mechanisms;object detection;road traffic;road vehicles;traffic engineering computing;video signal processing;AOG;and-or graph;bottom-up inference;congested traffic condition;contrast experiment;hierarchical decomposition;occlusion problem;time-of-day condition;urban traffic video monitoring system;vehicle detection;vehicle feature selection;vehicle object representation;vehicle occlusion;vehicle pose;vehicle shape;weather condition;Feature extraction;Image edge detection;Monitoring;Object detection;Training;Vehicle detection;Vehicles;Active basis model (ABM);and –or graph (AOG);bottom-up inference;maximally stable extremal region (MSER);vehicle detection}
}

@Article{7192655,
  Title                    = {Moving Object Classification Using a Combination of Static Appearance Features and Spatial and Temporal Entropy Values of Optical Flows},
  Author                   = {C. W. Liang and C. F. Juang},
  Journal                  = {IEEE Transactions on Intelligent Transportation Systems},
  Year                     = {2015},

  Month                    = {Dec},
  Number                   = {6},
  Pages                    = {3453-3464},
  Volume                   = {16},

  Abstract                 = {This paper proposes a new approach for classifying four types of moving objects in an intelligent transportation system. Pedestrians, cars, motorcycles, and bicycles are classified based on their side views from a fixed camera. A moving object is segmented and tracked using background subtraction, silhouette projection, an area ratio, a Kalman filter, and appearance correlation operations. For the classification of a segmented object, a combination of static and spatiotemporal features based on the cooccurrence of its appearance and the movements of its local parts is proposed. To extract the static appearance features, adaptive block-based gradient intensities and histograms of oriented gradients are proposed. For the spatiotemporal features, the optical-flow-based entropy values of instantaneous and short-term movements are proposed. The former finds the spatial entropy values of the orientations and the amplitudes of optical flows in a block to extract the local movement information from two consecutive image frames. The latter finds the temporal entropy values of the tracked optical flows in different orientation bins to extract the short-term movement information from several consecutive frames. Linear support vector machines with batch incremental learning are proposed to classify the four classes of objects. Experimental results from 12 test video sequences and comparisons with several feature descriptors show the effect of the proposed classification system and the advantage of the proposed features in classification.},
  Doi                      = {10.1109/TITS.2015.2459917},
  ISSN                     = {1524-9050},
  Keywords                 = {Feature extraction;Histograms;Object segmentation;Spatiotemporal phenomena;Support vector machines;Moving object segmentation;car detection;multi-class object classification;pedestrian detection;spatio-temporal features;support vector machines}
}

@Article{7100903,
  Title                    = {Counting and Classification of Highway Vehicles by Regression Analysis},
  Author                   = {M. Liang and X. Huang and C. H. Chen and X. Chen and A. Tokuta},
  Journal                  = {IEEE Transactions on Intelligent Transportation Systems},
  Year                     = {2015},

  Month                    = {Oct},
  Number                   = {5},
  Pages                    = {2878-2888},
  Volume                   = {16},

  Abstract                 = {In this paper, we describe a novel algorithm that counts and classifies highway vehicles based on regression analysis. This algorithm requires no explicit segmentation or tracking of individual vehicles, which is usually an important part of many existing algorithms. Therefore, this algorithm is particularly useful when there are severe occlusions or vehicle resolution is low, in which extracted features are highly unreliable. There are mainly two contributions in our proposed algorithm. First, a warping method is developed to detect the foreground segments that contain unclassified vehicles. The common used modeling and tracking (e.g., Kalman filtering) of individual vehicles are not required. In order to reduce vehicle distortion caused by the foreshortening effect, a nonuniform mesh grid and a projective transformation are estimated and applied during the warping process. Second, we extract a set of low-level features for each foreground segment and develop a cascaded regression approach to count and classify vehicles directly, which has not been used in the area of intelligent transportation systems. Three different regressors are designed and evaluated. Experiments show that our regression-based algorithm is accurate and robust for poor quality videos, from which many existing algorithms could fail to extract reliable features.},
  Doi                      = {10.1109/TITS.2015.2424917},
  ISSN                     = {1524-9050},
  Keywords                 = {image classification;image resolution;intelligent transportation systems;regression analysis;road vehicles;video signal processing;cascaded regression approach;extracted features;foreground segments;foreshortening effect;highway vehicle classification;intelligent transportation systems;low-level features;nonuniform mesh grid;occlusions;poor quality videos;projective transformation;regression analysis;regression-based algorithm;unclassified vehicles;vehicle distortion;vehicle resolution;Algorithm design and analysis;Feature extraction;Image segmentation;Roads;Vehicles;Videos;Highway vehicle;cascaded regression;image warping}
}

@Article{7576700,
  Title                    = {A Video-Based System for Vehicle Speed Measurement in Urban Roadways},
  Author                   = {D. C. Luvizon and B. T. Nassu and R. Minetto},
  Journal                  = {IEEE Transactions on Intelligent Transportation Systems},
  Year                     = {2016},
  Number                   = {99},
  Pages                    = {1-12},
  Volume                   = {PP},

  Abstract                 = {In this paper, we propose a nonintrusive video-based system for vehicle speed measurement in urban roadways. Our system uses an optimized motion detector and a novel text detector to efficiently locate vehicle license plates in image regions containing motion. Distinctive features are then selected on the license plate regions, tracked across multiple frames, and rectified for perspective distortion. Vehicle speed is measured by comparing the trajectories of the tracked features to known real-world measures. The proposed system was tested on a data set containing approximately 5 h of videos recorded in different weather conditions by a single low-cost camera, with associated ground truth speeds obtained by an inductive loop detector. Our data set is freely available for research purposes. The measured speeds have an average error of −0.5 km/h, staying inside the [−3, +2] km/h limit determined by regulatory authorities in several countries in over 96.0% of the cases. To the authors' knowledge, there are no other video-based systems able to achieve results comparable to those produced by an inductive loop detector. We also show that our license plate detector outperforms two other published state-of-the-art text detectors, as well as a well-known license plate detector, achieving a precision of 0.93 and a recall of 0.87.},
  Doi                      = {10.1109/TITS.2016.2606369},
  ISSN                     = {1524-9050},
  Keywords                 = {Detectors;Feature extraction;Licenses;Motion detection;Tracking;Vehicles;Velocity measurement;feature tracking;license plate detection;vehicle motion detection;vehicle speed measurement}
}

@Article{1603550,
  Title                    = {Video-based lane estimation and tracking for driver assistance: survey, system, and evaluation},
  Author                   = {J. C. McCall and M. M. Trivedi},
  Journal                  = {IEEE Transactions on Intelligent Transportation Systems},
  Year                     = {2006},

  Month                    = {March},
  Number                   = {1},
  Pages                    = {20-37},
  Volume                   = {7},

  Abstract                 = {Driver-assistance systems that monitor driver intent, warn drivers of lane departures, or assist in vehicle guidance are all being actively considered. It is therefore important to take a critical look at key aspects of these systems, one of which is lane-position tracking. It is for these driver-assistance objectives that motivate the development of the novel "video-based lane estimation and tracking" (VioLET) system. The system is designed using steerable filters for robust and accurate lane-marking detection. Steerable filters provide an efficient method for detecting circular-reflector markings, solid-line markings, and segmented-line markings under varying lighting and road conditions. They help in providing robustness to complex shadowing, lighting changes from overpasses and tunnels, and road-surface variations. They are efficient for lane-marking extraction because by computing only three separable convolutions, we can extract a wide variety of lane markings. Curvature detection is made more robust by incorporating both visual cues (lane markings and lane texture) and vehicle-state information. The experiment design and evaluation of the VioLET system is shown using multiple quantitative metrics over a wide variety of test conditions on a large test path using a unique instrumented vehicle. A justification for the choice of metrics based on a previous study with human-factors applications as well as extensive ground-truth testing from different times of day, road conditions, weather, and driving scenarios is also presented. In order to design the VioLET system, an up-to-date and comprehensive analysis of the current state of the art in lane-detection research was first performed. In doing so, a comparison of a wide variety of methods, pointing out the similarities and differences between methods as well as when and where various methods are most useful, is presented},
  Doi                      = {10.1109/TITS.2006.869595},
  ISSN                     = {1524-9050},
  Keywords                 = {computer vision;driver information systems;object detection;road vehicles;video signal processing;circular reflector markings;curvature detection;driver assistance system;driver monitoring;lane position tracking;machine vision;steerable filters;video lane estimation and tracking system;Data mining;Filters;Monitoring;Navigation;Roads;Robustness;Shadow mapping;System testing;Vehicle detection;Vehicle driving;Active safety systems;intelligent vehicles;lane detection;lane-departure warning;machine vision;performance metrics}
}

@Article{1637674,
  Title                    = {Detection and classification of highway lanes using vehicle motion trajectories},
  Author                   = {J. Melo and A. Naftel and A. Bernardino and J. Santos-Victor},
  Journal                  = {IEEE Transactions on Intelligent Transportation Systems},
  Year                     = {2006},

  Month                    = {June},
  Number                   = {2},
  Pages                    = {188-200},
  Volume                   = {7},

  Abstract                 = {Intelligent vision-based traffic surveillance systems are assuming an increasingly important role in highway monitoring and road management schemes. This paper describes a low-level object tracking system that produces accurate vehicle motion trajectories that can be further analyzed to detect lane centers and classify lane types. Accompanying techniques for indexing and retrieval of anomalous trajectories are also derived. The predictive trajectory merge-and-split algorithm is used to detect partial or complete occlusions during object motion and incorporates a Kalman filter that is used to perform vehicle tracking. The resulting motion trajectories are modeled using variable low-degree polynomials. A K-means clustering technique on the coefficient space can be used to obtain approximate lane centers. Estimation bias due to vehicle lane changes can be removed using robust estimation techniques based on Random Sample Consensus (RANSAC). Through the use of nonmetric distance functions and a simple directional indicator, highway lanes can be classified into one of the following categories: entry, exit, primary, or secondary. Experimental results are presented to show the real-time application of this approach to multiple views obtained by an uncalibrated pan-tilt-zoom traffic camera monitoring the junction of two busy intersecting highways.},
  Doi                      = {10.1109/TITS.2006.874706},
  ISSN                     = {1524-9050},
  Keywords                 = {Kalman filters;automated highways;image motion analysis;object detection;pattern clustering;polynomials;road traffic;surveillance;Kalman filter;highway lanes classification;highway lanes detection;intelligent vision-based traffic surveillance systems;k-means clustering technique;low-degree polynomials;nonmetric distance functions;object tracking system;random sample consensus;road management;vehicle motion trajectories;Monitoring;Motion analysis;Motion detection;Object detection;Road transportation;Road vehicles;Surveillance;Tracking;Trajectory;Vehicle detection;Lane detection;motion trajectory;scene interpretation;vehicle tracking}
}

@Article{6172683,
  Title                    = {On-Road Multivehicle Tracking Using Deformable Object Model and Particle Filter With Improved Likelihood Estimation},
  Author                   = {H. Tehrani Niknejad and A. Takeuchi and S. Mita and D. McAllester},
  Journal                  = {IEEE Transactions on Intelligent Transportation Systems},
  Year                     = {2012},

  Month                    = {June},
  Number                   = {2},
  Pages                    = {748-758},
  Volume                   = {13},

  Abstract                 = {This paper proposes a novel method for multivehicle detection and tracking using a vehicle-mounted monocular camera. In the proposed method, the features of vehicles are learned as a deformable object model through the combination of a latent support vector machine (LSVM) and histograms of oriented gradients (HOGs). The detection algorithm combines both global and local features of the vehicle as a deformable object model. Detected vehicles are tracked through a particle filter, which estimates the particles' likelihood by using a detection scores map and template compatibility for both root and parts of the vehicle while considering the deformation cost caused by the movement of vehicle parts. Tracking likelihoods are iteratively used as a priori probability to generate vehicle hypothesis regions and update the detection threshold to reduce false negatives of the algorithm presented before. Extensive experiments in urban scenarios showed that the proposed method can achieve an average vehicle detection rate of 97% and an average vehicle-tracking rate of 86% with a false positive rate of less than 0.26%.},
  Doi                      = {10.1109/TITS.2012.2187894},
  ISSN                     = {1524-9050},
  Keywords                 = {cameras;gradient methods;object detection;particle filtering (numerical methods);support vector machines;traffic engineering computing;vehicles;HOG;LSVM;deformable object model;deformation cost;detection algorithm;detection scores;global features;histograms of oriented gradients;latent support vector machine;likelihood estimation;likelihoods tracking;local features;multivehicle detection;on-road multivehicle tracking;particle filter;template compatibility;vehicle hypothesis regions;vehicle parts;vehicle-mounted monocular camera;Computational modeling;Correlation;Deformable models;Image color analysis;Tracking;Vectors;Vehicles;Intelligent vehicles;object detection;pattern recognition;tracking filters}
}

@Article{5373840,
  Title                    = {Vision--IMU Integration Using a Slow-Frame-Rate Monocular Vision System in an Actual Roadway Setting},
  Author                   = {D. I. B. Randeniya and S. Sarkar and M. Gunaratne},
  Journal                  = {IEEE Transactions on Intelligent Transportation Systems},
  Year                     = {2010},

  Month                    = {June},
  Number                   = {2},
  Pages                    = {256-266},
  Volume                   = {11},

  Abstract                 = {We present results of an effort where position and orientation data from vision and inertial sensors are integrated and validated using data from an actual roadway. Information from a sequence of images, which were captured by a monocular camera attached to a survey vehicle at a maximum frequency of 3 frames/s, is fused with position and orientation estimates from the inertial system to correct for inherent error accumulation in such integral-based systems. The rotations and translations are estimated from point correspondences tracked through a sequence of images. To reduce unsuitable correspondences, we used constraints such as epipolar lines and correspondence flow directions. The vision algorithm automatically operates and involves the identification of point correspondences, the pruning of correspondences, and the estimation of motion parameters. To simply obtain the geodetic coordinates, i.e., latitude, longitude, and altitude, from the translation-direction estimates from the vision sensor, we expand the Kalman filter space to incorporate distance. Hence, it was possible to extract the translational vector from the available translational direction estimate of the vision system. Finally, a decentralized Kalman filter is used to integrate the position estimates based on the vision sensor with those of the inertial system. The fusion of the two sensors was carried out at the system level in the model. The comparison of integrated vision-inertial-measuring-unit (IMU) position estimates with those from inertial-GPS system output and actual survey demonstrates that vision sensing can be used to reduce errors in inertial measurements during potential GPS outages.},
  Doi                      = {10.1109/TITS.2009.2038276},
  ISSN                     = {1524-9050},
  Keywords                 = {Kalman filters;automated highways;computer vision;image sensors;motion estimation;Kalman filter space;actual roadway setting;flow directions;inertial sensors;integral based systems;monocular camera;motion estimation;position estimation;slow frame rate monocular vision system;survey vehicle;vision IMU integration;vision inertial measuring unit;vision sensors;Inertial navigation;intelligent vehicles;multisensor fusion;vision–inertial–Globall Positioning System (GPS) fusion}
}

@Article{7101268,
  Title                    = {Robust Vehicle Detection and Distance Estimation Under Challenging Lighting Conditions},
  Author                   = {M. Rezaei and M. Terauchi and R. Klette},
  Journal                  = {IEEE Transactions on Intelligent Transportation Systems},
  Year                     = {2015},

  Month                    = {Oct},
  Number                   = {5},
  Pages                    = {2723-2743},
  Volume                   = {16},

  Abstract                 = {Avoiding high computational costs and calibration issues involved in stereo-vision-based algorithms, this paper proposes real-time monocular-vision-based techniques for simultaneous vehicle detection and inter-vehicle distance estimation, in which the performance and robustness of the system remain competitive, even for highly challenging benchmark datasets. This paper develops a collision warning system by detecting vehicles ahead and, by identifying safety distances to assist a distracted driver, prior to occurrence of an imminent crash. We introduce adaptive global Haar-like features for vehicle detection, tail-light segmentation, virtual symmetry detection, intervehicle distance estimation, as well as an efficient single-sensor multifeature fusion technique to enhance the accuracy and robustness of our algorithm. The proposed algorithm is able to detect vehicles ahead at both day or night and also for short- and long-range distances. Experimental results under various weather and lighting conditions (including sunny, rainy, foggy, or snowy) show that the proposed algorithm outperforms state-of-the-art algorithms.},
  Doi                      = {10.1109/TITS.2015.2421482},
  ISSN                     = {1524-9050},
  Keywords                 = {computer vision;environmental factors;image fusion;image segmentation;image sensors;real-time systems;traffic engineering computing;adaptive global Haar-like features;collision warning system;foggy;intervehicle distance estimation;lighting conditions;long-range distances;rainy;real-time monocular-vision-based techniques;robust vehicle detection;short-range distances;single-sensor multifeature fusion technique;snowy;sunny;tail-light segmentation;virtual symmetry detection;weather conditions;Estimation;Feature extraction;Lighting;Roads;Sensors;Vehicle detection;Vehicles;Advanced driver assistance systems;Dempster–Shafer fusion;Dempster???Shafer fusion;challenging lighting condition;collision avoidance;distance estimation;global Haar-like features;horizontal edge detection;rear-end crashes;symmetry detection;tail-light segmentation;vehicle detection}
}

@Article{1253217,
  Title                    = {Dynamic camera calibration of roadside traffic management cameras for vehicle speed estimation},
  Author                   = {T. N. Schoepflin and D. J. Dailey},
  Journal                  = {IEEE Transactions on Intelligent Transportation Systems},
  Year                     = {2003},

  Month                    = {June},
  Number                   = {2},
  Pages                    = {90-98},
  Volume                   = {4},

  Abstract                 = {In this paper, we present a new three-stage algorithm to calibrate roadside traffic management cameras and track vehicles to create a traffic speed sensor. The algorithm first estimates the camera position relative to the roadway using the motion and edges of the vehicles. Given the camera position, the algorithm then calibrates the camera by estimating the lane boundaries and the vanishing point of the lines along the roadway. The algorithm transforms the image coordinates from the vehicle tracker into real-world coordinates using our simplified camera model. We present results that demonstrate the ability of our algorithm to produce good estimates of the mean vehicle speed in a lane of traffic.},
  Doi                      = {10.1109/TITS.2003.821213},
  ISSN                     = {1524-9050},
  Keywords                 = {calibration;cameras;computer vision;image sensors;motion estimation;road traffic;velocity measurement;camera position estimation;dynamic camera calibration;image coordinates;lane boundaries;roadside traffic management cameras;roadways;traffic speed sensors;vanishing points;vehicle speed estimation;vehicle tracking;Calibration;Cameras;Feature extraction;Image processing;Layout;Motion estimation;Road vehicles;Traffic control;Vehicle dynamics;Velocity measurement}
}

@Article{7289400,
  Title                    = {Real-Time Traffic Light Detection With Adaptive Background Suppression Filter},
  Author                   = {Z. Shi and Z. Zou and C. Zhang},
  Journal                  = {IEEE Transactions on Intelligent Transportation Systems},
  Year                     = {2016},

  Month                    = {March},
  Number                   = {3},
  Pages                    = {690-700},
  Volume                   = {17},

  Abstract                 = {Traffic light detection plays an important role in intelligent transportation system, and many detection methods have been proposed in recent years. However, illumination variation effect is still of its major technical problem in real urban driving environments. In this paper, we propose a novel vision-based traffic light detection method for driving vehicles, which is fast and robust under different illumination conditions. The proposed method contains two stages: the candidate extraction stage and the recognition stage. On the candidate extraction stage, we propose an adaptive background suppression algorithm to highlight the traffic light candidate regions while suppressing the undesired backgrounds. On the recognition stage, each candidate region is verified and is further classified into different traffic light semantic classes. We evaluate our method on video sequences (more than 5000 frames and labels) captured from urban streets and suburb roads in varying illumination and compared with other vision-based traffic detection approaches. The experiment shows that the proposed method can achieve a desired detection result with high quality and robustness; simultaneously, the whole detection system can meet the real-time processing requirement of about 15 fps on video sequences.},
  Doi                      = {10.1109/TITS.2015.2481459},
  ISSN                     = {1524-9050},
  Keywords                 = {computer vision;filters;image recognition;road traffic;transportation;adaptive background suppression algorithm;adaptive background suppression filter;driving vehicles;illumination variation effect;intelligent transportation system;real-time processing;real-time traffic light detection;suburb roads;urban streets;video sequences;vision-based traffic detection approaches;vision-based traffic light detection method;Feature extraction;Histograms;Image color analysis;Lighting;Real-time systems;Robustness;Shape;Traffic light detection;adaptive background suppression filter;support vector machine}
}

@Article{Sivaraman2013,
  author   = {S. Sivaraman and M. M. Trivedi},
  title    = {Looking at Vehicles on the Road: A Survey of Vision-Based Vehicle Detection, Tracking, and Behavior Analysis},
  journal  = {IEEE Transactions on Intelligent Transportation Systems},
  year     = {2013},
  volume   = {14},
  number   = {4},
  pages    = {1773-1795},
  month    = {Dec},
  issn     = {1524-9050},
  abstract = {This paper provides a review of the literature in on-road vision-based vehicle detection, tracking, and behavior understanding. Over the past decade, vision-based surround perception has progressed from its infancy into maturity. We provide a survey of recent works in the literature, placing vision-based vehicle detection in the context of sensor-based on-road surround analysis. We detail advances in vehicle detection, discussing monocular, stereo vision, and active sensor-vision fusion for on-road vehicle detection. We discuss vision-based vehicle tracking in the monocular and stereo-vision domains, analyzing filtering, estimation, and dynamical models. We discuss the nascent branch of intelligent vehicles research concerned with utilizing spatiotemporal measurements, trajectories, and various features to characterize on-road behavior. We provide a discussion on the state of the art, detail common performance metrics and benchmarks, and provide perspective on future research directions in the field.},
  doi      = {10.1109/TITS.2013.2266661},
  keywords = {automated highways;behavioural sciences;computer vision;filtering theory;image fusion;object detection;object tracking;road vehicles;stereo image processing;active sensor-vision fusion;behavior understanding;dynamical models;filtering analysis;intelligent vehicle research;monocular vision domains;on-road behavior analysis;on-road vision-based vehicle detection;sensor-based on-road surround analysis;spatiotemporal measurements;stereo-vision domains;vision-based surround perception;vision-based vehicle tracking;Computer vision;Intelligent vehicles;Machine learning;Object detection;Object tracking;Computer vision;intelligent vehicles;machine learning;object detection;object tracking},
}

@Article{5671488,
  Title                    = {Automatic Road Environment Classification},
  Author                   = {I. Tang and T. P. Breckon},
  Journal                  = {IEEE Transactions on Intelligent Transportation Systems},
  Year                     = {2011},

  Month                    = {June},
  Number                   = {2},
  Pages                    = {476-484},
  Volume                   = {12},

  Abstract                 = {The ongoing development autonomous vehicles and adaptive vehicle dynamics present in many modern vehicles has generated a need for road environment classification - i.e., the ability to determine the nature of the current road or terrain environment from an onboard vehicle sensor. In this paper, we investigate the use of a low-cost camera vision solution capable of urban, rural, or off-road classification based on the analysis of color and texture features extracted from a driver's perspective camera view. A feature set based on color and texture distributions is extracted from multiple regions of interest in this forward-facing camera view and combined with a trained classifier approach to resolve two road-type classification problems of varying difficulty - {off-road, on-road} environment determination and the additional multiclass road environment problem of {off-road, urban, major/trunk road and multilane motorway/carriageway}. Two illustrative classification approaches are investigated, and the results are reported over a series of real environment data. An optimal performance of ~90% correct classification is achieved for the {off-road, on-road} problem at a near real-time classification rate of 1 Hz.},
  Doi                      = {10.1109/TITS.2010.2095499},
  ISSN                     = {1524-9050},
  Keywords                 = {cameras;feature extraction;image classification;image colour analysis;image sensors;image texture;learning (artificial intelligence);real-time systems;road vehicles;traffic engineering computing;adaptive vehicle dynamics;automatic road environment classification;autonomous vehicle;color analysis;feature extraction;forward-facing camera view;low-cost camera vision;machine learning;multiclass road environment;multilane carriageway;multilane motorway;off-road classification;off-road environment;on-road environment;onboard vehicle sensor;real-time classification rate;road-type classification problem;rural classification;terrain environment;texture analysis;trunk road;urban classification;Artificial neural networks;Feature extraction;Image color analysis;Image edge detection;Roads;Training;Video sequences;Color classification;machine learning classifier;road-type classification;texture classification}
}

@Article{5166486,
  Title                    = {Learning to Recognize Video-Based Spatiotemporal Events},
  Author                   = {H. Veeraraghavan and N. P. Papanikolopoulos},
  Journal                  = {IEEE Transactions on Intelligent Transportation Systems},
  Year                     = {2009},

  Month                    = {Dec},
  Number                   = {4},
  Pages                    = {628-638},
  Volume                   = {10},

  Abstract                 = {A key research issue in activity recognition in real-world applications, such as in intelligent transportation systems (ITS), is to automatically learn robust models of activities that require minimal human training. In this paper, we contribute a novel approach for learning sequenced spatiotemporal activities in outdoor traffic intersections. Concretely, by representing the activities as sequences of actions, we contribute a semisupervised learning algorithm that learns activities as complete stochastic context-free grammars (SCFGs), namely, the grammar structure and the parameters. Our approach has been implemented and tested on real-world scenes, and we present experimental results of the grammar learning and activity recognition applied to data collection and traffic monitoring applications using video data.},
  Doi                      = {10.1109/TITS.2009.2026440},
  ISSN                     = {1524-9050},
  Keywords                 = {automated highways;context-free grammars;learning (artificial intelligence);video signal processing;grammar learning;intelligent transportation systems;outdoor traffic intersections;semisupervised learning algorithm;stochastic context-free grammars;video based spatiotemporal events;video recognition;Context-free grammars;intelligent transportation system (ITS) applications;machine learning;vehicle tracking;video analysis}
}

@Article{6166893,
  Title                    = {Real-Time Computer Vision/DGPS-Aided Inertial Navigation System for Lane-Level Vehicle Navigation},
  Author                   = {A. Vu and A. Ramanandan and A. Chen and J. A. Farrell and M. Barth},
  Journal                  = {IEEE Transactions on Intelligent Transportation Systems},
  Year                     = {2012},

  Month                    = {June},
  Number                   = {2},
  Pages                    = {899-913},
  Volume                   = {13},

  Abstract                 = {Many intelligent transportation system (ITS) applications will increasingly rely on lane-level vehicle positioning that requires high accuracy, bandwidth, availability, and integrity. Lane-level positioning methods must reliably work in real time in a wide range of environments, spanning rural to urban areas. Traditional positioning sensors such as the Global Navigation Satellite Systems may have poor performance in dense urban areas, where obstacles block satellite signals. This paper presents a sensor fusion technique that uses computer vision and differential pseudorange Global Positioning System (DGPS) measurements to aid an inertial navigation system (INS) in challenging environments where GPS signals are limited and/or unreliable. To supplement limited DGPS measurements, this method uses mapped landmarks that were measured through a priori observations (e.g., traffic light location data), taking advantage of existing infrastructure that is abundant within suburban/urban environments. For example, traffic lights are easily detected by color vision sensors in both day and night conditions. A tightly coupled estimation process is employed to use observables from satellite signals and known feature observables from a camera to correct an INS that is formulated as an extended Kalman filter. A traffic light detection method is also outlined, where the projected feature uncertainty ellipse is utilized to perform data association between a predicted feature and a set of detected features. Real-time experimental results from real-world settings are presented to validate the proposed localization method.},
  Doi                      = {10.1109/TITS.2012.2187641},
  ISSN                     = {1524-9050},
  Keywords                 = {Global Positioning System;Kalman filters;computer vision;feature extraction;image fusion;inertial navigation;object detection;road traffic;traffic engineering computing;DGPS measurement;DGPS-aided inertial navigation system;data association;differential pseudorange Global Positioning System;extended Kalman filter;feature detection;feature uncertainty ellipse;global navigation satellite systems;intelligent transportation system;lane-level vehicle navigation;lane-level vehicle positioning method;mapped landmark;positioning sensor;realtime computer vision;sensor fusion technique;suburban environment;tightly coupled estimation process;traffic light detection method;traffic light location data;urban environment;Cameras;Global Positioning System;Real time systems;Satellites;Sensors;Vectors;Vehicles;Advance vehicle control systems;advance vehicle safety systems;aided navigation;feature detection;image/video aiding;inertial navigation;intelligent transportation systems (ITS);land transportation;localization;sensor fusion}
}

@Article{6645460,
  Title                    = {EasiSee: Real-Time Vehicle Classification and Counting via Low-Cost Collaborative Sensing},
  Author                   = {R. Wang and L. Zhang and K. Xiao and R. Sun and L. Cui},
  Journal                  = {IEEE Transactions on Intelligent Transportation Systems},
  Year                     = {2014},

  Month                    = {Feb},
  Number                   = {1},
  Pages                    = {414-424},
  Volume                   = {15},

  Abstract                 = {In the field of traffic-information acquisition, one pervasive solution is to use wireless sensor networks (WSNs) to realize vehicle classification and counting. By adopting heterogeneous sensors in a WSN, we can explore the potential of using complementary physical information to perform more complicated sensing computation. However, the collaboration among heterogeneous sensors, such as the collaborative sensing mechanism (CSM), is not well studied in current state-of-the-art research. In this paper, we design and implement EasiSee, a real-time vehicle classification and counting system based on WSNs. Our contributions are as follows. First, we propose a CSM, which coordinates the power-hungry camera sensor and the power-efficient magnetic sensors, reducing the overall system energy consumption and maximizing system lifetime. Second, we propose a robust vehicle image-processing algorithm, i.e., a low-cost image processing algorithm (LIPA). LIPA reduces environment noise and interference with low computation complexity. In the verification section, the vehicle detection accuracy turned out to be 95.31%, which pave the way for CSM. The time of image processing is around 200 ms, which indicates that our LIPA is computationally economical. With the overall energy consumption reduced, EasiSee achieves classification accuracy of 93%. Based on these experiments and analysis, we conclude that EasiSee is a practical and low-cost affordable solution for traffic-information acquisition.},
  Doi                      = {10.1109/TITS.2013.2281760},
  ISSN                     = {1524-9050},
  Keywords                 = {cameras;image processing;magnetic sensors;power consumption;real-time systems;road traffic;road vehicles;wireless sensor networks;CSM;EasiSee;LIPA;WSN;collaborative sensing mechanism;complementary physical information;computation complexity;energy consumption;environment noise;heterogeneous sensors;low-cost collaborative sensing;low-cost image processing;power-efficient magnetic sensors;power-hungry camera sensor;real-time vehicle classification;real-time vehicle counting;traffic-information acquisition;wireless sensor networks;Cameras;Image sensors;Magnetic sensors;Vehicle detection;Vehicles;Wireless sensor networks;Collaborative sensing;low cost;real time;wireless sensor networks (WSN)}
}

@Article{7605450,
  Title                    = {An Incremental Framework for Video-Based Traffic Sign Detection, Tracking, and Recognition},
  Author                   = {Y. Yuan and Z. Xiong and Q. Wang},
  Journal                  = {IEEE Transactions on Intelligent Transportation Systems},
  Year                     = {2016},
  Number                   = {99},
  Pages                    = {1-12},
  Volume                   = {PP},

  Abstract                 = {Video-based traffic sign detection, tracking, and recognition is one of the important components for the intelligent transport systems. Extensive research has shown that pretty good performance can be obtained on public data sets by various state-of-the-art approaches, especially the deep learning methods. However, deep learning methods require extensive computing resources. In addition, these approaches mostly concentrate on single image detection and recognition task, which is not applicable in real-world applications. Different from previous research, we introduce a unified incremental computational framework for traffic sign detection, tracking, and recognition task using the mono-camera mounted on a moving vehicle under non-stationary environments. The main contributions of this paper are threefold: 1) to enhance detection performance by utilizing the contextual information, this paper innovatively utilizes the spatial distribution prior of the traffic signs; 2) to improve the tracking performance and localization accuracy under non-stationary environments, a new efficient incremental framework containing off-line detector, online detector, and motion model predictor together is designed for traffic sign detection and tracking simultaneously; and 3) to get a more stable classification output, a scale-based intra-frame fusion method is proposed. We evaluate our method on two public data sets and the performance has shown that the proposed system can obtain results comparable with the deep learning method with less computing resource in a near-real-time manner.},
  Doi                      = {10.1109/TITS.2016.2614548},
  ISSN                     = {1524-9050},
  Keywords                 = {Color;Detectors;Image color analysis;Machine learning;Shape;Target tracking;ITS.;Machine learning;detection;incremental learning;recognition;tracking;traffic sign}
}

@Article{6671929,
  Title                    = {A Practical Roadside Camera Calibration Method Based on Least Squares Optimization},
  Author                   = {Y. Zheng and S. Peng},
  Journal                  = {IEEE Transactions on Intelligent Transportation Systems},
  Year                     = {2014},

  Month                    = {April},
  Number                   = {2},
  Pages                    = {831-843},
  Volume                   = {15},

  Abstract                 = {In this paper, we propose a more practical and accurate method for calibrating the roadside camera used in traffic surveillance systems. Considering the characteristics of the traffic scenes, we propose a minimum calibration condition that consists of two vanishing points and a vanishing line, which can be easily satisfied in most traffic scenes. Based on the minimum calibration condition, we provide a calibration method to estimate camera intrinsic parameters and rotation angles, which employs least squares optimization instead of closed-form computation. Compared with the existing calibration methods, our method is suitable for more traffic scenes and is able to accurately determine more camera parameters including the principal point. By making full use of video information, multiple observations of the vanishing points are available from different objects. For more accurate calibration, we present a dynamic calibration method using these observations to correct camera parameters. As for the estimation of the camera translation vector, known lengths in the road or known heights above the road are exploited. The experimental results on synthetic data and real traffic images demonstrate the accuracy, robustness, and practicability of the proposed calibration method.},
  Doi                      = {10.1109/TITS.2013.2288353},
  ISSN                     = {1524-9050},
  Keywords                 = {calibration;cameras;intelligent transportation systems;least squares approximations;road traffic;video surveillance;camera intrinsic parameter estimation;camera rotation angles;camera translation vector estimation;least squares optimization;practical roadside camera calibration method;traffic surveillance systems;Calibration;Cameras;Estimation;Optimization;Roads;Surveillance;Vectors;Camera calibration;least squares (LS) optimization;roadside camera;traffic surveillance;vanishing line;vanishing point}
}

@Article{7551138,
  Title                    = {An Adaptive Background Modeling Method for Foreground Segmentation},
  Author                   = {Z. Zhong and B. Zhang and G. Lu and Y. Zhao and Y. Xu},
  Journal                  = {IEEE Transactions on Intelligent Transportation Systems},
  Year                     = {2016},
  Number                   = {99},
  Pages                    = {1-13},
  Volume                   = {PP},

  Abstract                 = {Background modeling has played an important role in detecting the foreground for video analysis. In this paper, we presented a novel background modeling method for foreground segmentation. The innovations of the proposed method lie in the joint usage of the pixel-based adaptive segmentation method and the background updating strategy, which is performed in both pixel and object levels. Current pixel-based adaptive segmentation method only updates the background at the pixel level and does not take into account the physical changes of the object, which may result in a series of problems in foreground detection, e.g., a static or low-speed object is updated too fast or merely a partial foreground region is properly detected. To avoid these deficiencies, we used a counter to place the foreground pixels into two categories (illumination and object). The proposed method extracted a correct foreground object by controlling the updating time of the pixels belonging to an object or an illumination region respectively. Extensive experiments showed that our method is more competitive than the state-of-the-art foreground detection methods, particularly in the intermittent object motion scenario. Moreover, we also analyzed the efficiency of our method in different situations to show that the proposed method is available for real-time applications.},
  Doi                      = {10.1109/TITS.2016.2597441},
  ISSN                     = {1524-9050},
  Keywords                 = {Adaptation models;Intelligent transportation systems;Lighting;Motion segmentation;Object detection;Shape;Surveillance;Foreground segmentation;adaptive background updating;background modeling}
}

@InProceedings{Boyle2015,
  author        = {J. Boyle and J. Ferryman},
  title         = {Vehicle subtype, make and model classification from side profile video},
  booktitle     = {Advanced Video and Signal Based Surveillance (AVSS), 2015 12th IEEE International Conference on},
  year          = {2015},
  pages         = {1-6},
  month         = {Aug},
  __markedentry = {[Barancsuk Lilla:6]},
  abstract      = {This paper addresses the challenging domain of vehicle classification from pole-mounted roadway cameras, specifically from side-profile views. A new public vehicle dataset is made available consisting of over 10000 side profile images (86 make/model and 9 sub-type classes). 5 state-of-the-art classifiers are applied to the dataset, with the best achieving high classification rates of 98.7% for sub-type and 99.7-99.9% for make and model recognition, confirming the assertion made that single vehicle side profile images can be used for robust classification.},
  doi           = {10.1109/AVSS.2015.7301783},
  keywords      = {image classification;road vehicles;traffic engineering computing;video signal processing;model recognition;pole-mounted roadway cameras;robust classification;side profile video;single vehicle side profile images;vehicle classification;vehicle subtype;Cameras;Image color analysis;Support vector machines;Training;Vegetation;Vehicles;Wheels},
}

@InProceedings{Bragatto2008,
  author        = {T. A. C. Bragatto and G. I. S. Ruas and V. A. P. Benso and M. V. Lamar and D. Aldigueri and G. L. Teixeira and Y. Yamashita},
  title         = {A new approach to multiple vehicle tracking in intersections using harris corners and adaptive background subtraction},
  booktitle     = {Intelligent Vehicles Symposium, 2008 IEEE},
  year          = {2008},
  pages         = {548-553},
  month         = {June},
  __markedentry = {[Barancsuk Lilla:6]},
  abstract      = {This work presents a new methodology based on video processing for recording and counting vehicles in intersections and urban roads, allowing the control and management of urban traffic and support to inspection of public road security in large cities. The aim of this work is to show the viability of computer vision techniques for the construction of a portable equipment able to perform urban vehicles flow counting and classification into routes in an automatic way and in real time. Image processing techniques, such as background subtraction, definition and tracking of object features are used in the prototype development and implementation. The obtained results show the viability of use of the proposed portable monitoring system in real-time, achieving correct rates in the vehicle counting of about 96% for simple roads and 72% at complex intersections.},
  doi           = {10.1109/IVS.2008.4621293},
  issn          = {1931-0587},
  keywords      = {object detection;road traffic;road vehicles;tracking;traffic information systems;video signal processing;adaptive background subtraction;computer vision techniques;image processing techniques;multiple vehicle tracking;portable equipment;public road security;urban roads;urban traffic control;urban traffic management;urban vehicles classification;urban vehicles flow counting;video processing;Cameras;Cities and towns;Computer vision;Computerized monitoring;Control systems;Intelligent vehicles;Layout;Road vehicles;Tracking;Vehicle detection},
}

@InProceedings{Brulin2014,
  author        = {M. Brulin and C. Maillet and H. Nicolas},
  title         = {1-D temporal segments analysis for traffic video surveillance},
  booktitle     = {Computer Vision Theory and Applications (VISAPP), 2014 International Conference on},
  year          = {2014},
  volume        = {1},
  pages         = {557-563},
  month         = {Jan},
  __markedentry = {[Barancsuk Lilla:6]},
  abstract      = {Traffic video surveillance is an important topic for security purposes and to improve the traffic flow management. Video surveillance can be used for different purposes such as counting of vehicles or to detect their speed and behaviors. In this context, it is often important to be able to analyze the video in real-time. The huge amount of data generated by the increasing number of cameras is an obstacle to reach this goal. A solution consists in selecting in the video only the regions of interest, essentially the vehicles on the road areas. In this paper, we propose to extract significant segments of the regions of interest and to analyze them temporally to count vehicles and to define their behaviors. Experiments on real data show that precise vehicle's counting and high recall and precision are obtain for vehicle's behavior and traffic analysis.},
  keywords      = {Cameras;Image segmentation;Real-time systems;Roads;Streaming media;Vehicles;Video surveillance;Behavior;Temporal Segment;Traffic;Video Surveillance},
}

@Article{Chen2014,
  author        = {X. Chen and Y. Ruan and P. Zhang and Q. Chen and X. Zhang},
  title         = {Vehicle representation and classification of surveillance video based on sparse learning},
  journal       = {China Communications},
  year          = {2014},
  volume        = {11},
  number        = {13},
  pages         = {135-141},
  month         = {Supplement},
  issn          = {1673-5447},
  __markedentry = {[Barancsuk Lilla:6]},
  abstract      = {We cast vehicle recognition as problem of feature representation and classification, and introduce a sparse learning based framework for vehicle recognition and classification in this paper. After objects captured with a GMM background subtraction program, images are labeled with vehicle type for dictionary learning and decompose the images with sparse coding (SC), a linear SVM trained with the SC feature for vehicle classification. A simple but efficient active learning strategy is adopted by adding the false positive samples into previous training set for dictionary and SVM model retraining. Compared with traditional feature representation and classification realized with SVM, SC method achieves dramatically improvement on classification accuracy and exhibits strong robustness. The work is also validated on real-world surveillance video.},
  doi           = {10.1109/CC.2014.7022537},
  keywords      = {Gaussian processes;image classification;image coding;image representation;intelligent transportation systems;learning (artificial intelligence);mixture models;support vector machines;traffic engineering computing;video surveillance;GMM background subtraction program;Gaussian mixture model;SC method;SVM model retraining;active learning strategy;dictionary learning;false positive samples;linear SVM;sparse coding;sparse learning based framework;vehicle classification;vehicle recognition;vehicle representation;video surveillance;Accuracy;Classification algorithms;Dictionaries;Support vector machines;Surveillance;Training;Vehicles;feature representation;robustness and generalization;sparse learning;vehicle classification},
}

@Article{Chen2016,
  author        = {Y. W. Chen and K. Chen and S. Y. Yuan and S. Y. Kuo},
  title         = {Moving Object Counting Using a Tripwire in H.265/HEVC Bitstreams for Video Surveillance},
  journal       = {IEEE Access},
  year          = {2016},
  volume        = {4},
  pages         = {2529-2541},
  issn          = {2169-3536},
  __markedentry = {[Barancsuk Lilla:6]},
  abstract      = {The objective of this paper is to estimate the number of moving objects that passes through a specific area without fully decoding the H.265/high-efficiency video coding (HEVC) bitstreams. First, the foreground prediction blocks are extracted according to the motion vectors of the H.265/HEVC bitstreams. Next, these foreground prediction blocks are clustered into the region of interests (ROIs), which are the possible area position of moving objects in the current frame. Finally, the state of moving objects is identified by matching moving objects and these ROIs. In order to estimate the number of moving objects, which move toward a pre-defined direction, a tripwire is set to a detecting area. Any moving objects crossing the tripwire and satisfying the intrusion conditions are counted. With the proposed method, the number of moving objects can be directly estimated in the compressed domain video. This approach significantly increase the processing speed more than 400% at the cost of less than 0.02% accuracy degradation compared with the traditional pixel domain approach. The research results can be applied to traffic management, real-time analysis of surveillance application, and other related areas.},
  doi           = {10.1109/ACCESS.2016.2572121},
  keywords      = {Decoding;Object recognition;Surveillance;Traffic control;Video recording;H.265/HEVC;object counting;video surveillance applications},
}

@Article{Cheung2005,
  author        = {Cheung and S. Sen-Ching and C. KamathKamath},
  title         = {Robust Background Subtraction with Foreground Validation for Urban Traffic Video},
  journal       = {EURASIP J. Appl. Signal Process.},
  year          = {2005},
  volume        = {2005},
  pages         = {2330--2340},
  month         = jan,
  issn          = {1110-8657},
  __markedentry = {[Barancsuk Lilla:6]},
  acmid         = {1287286},
  address       = {New York, NY, United States},
  doi           = {10.1155/ASP.2005.2330},
  issue_date    = {1 January 2005},
  keywords      = {background subtraction, foreground validation, urban traffic video},
  numpages      = {11},
  publisher     = {Hindawi Publishing Corp.},
  url           = {http://dx.doi.org/10.1155/ASP.2005.2330},
}

@Article{Csorba2016,
  author        = {K. Csorba and L. Barancsuk and L. Blázovics},
  title         = {Visual Traffic Load Sensor for Emission Estimation},
  journal       = {Sensors},
  year          = {2016},
  note          = {Accepted for inclusion},
  __markedentry = {[Barancsuk Lilla:6]},
  owner         = {BarancsukLilla},
  timestamp     = {2016.10.03},
}

@Article{Engel2016,
  author        = {J. I. Engel and J. MartĂ­n and R. Barco},
  title         = {A Low-Complexity Vision-Based System for Real-Time Traffic Monitoring},
  journal       = {IEEE Transactions on Intelligent Transportation Systems},
  year          = {2016},
  volume        = {PP},
  number        = {99},
  pages         = {1-10},
  issn          = {1524-9050},
  __markedentry = {[Barancsuk Lilla:6]},
  abstract      = {In this paper a novel, efficient, and fast-performing vision-based system for traffic flow monitoring is presented. Using standard traffic surveillance cameras and effectively applying simple techniques, the proposed method can produce accurate results on vehicle counting in different challenging situations, such as low-resolution videos, rainy scenes, and situations of stop-and-go traffic. Due to the simplicity of the proposed algorithm, the system is able to manage multiple video streams simultaneously in real time. The method follows a robust adaptive background segmentation strategy based on the Approximated Median Filter technique, which detects pixels corresponding to moving objects. Experimental results show that the proposed method can achieve sufficient accuracy and reliability while showing high performance rates, outperforming other state-of-the-art methods. Tests have proved that the system is able to work with up to 50 standard-resolution cameras at the same time in a standard computer, producing satisfactory results.},
  doi           = {10.1109/TITS.2016.2603069},
  keywords      = {Cameras;Computers;Detectors;Monitoring;Real-time systems;Vehicles;Videos;Computer vision;image processing;traffic image analysis;traffic information systems},
}

@InProceedings{Hung2016,
  author        = {N. V. Hung and N. H. Dung and L. C. Tran and T. M. Hoang and N. T. Dzung},
  title         = {Vehicle classification by estimation of the direction angle in a mixed traffic flow},
  booktitle     = {2016 IEEE Sixth International Conference on Communications and Electronics (ICCE)},
  year          = {2016},
  pages         = {365-368},
  month         = {July},
  __markedentry = {[Barancsuk Lilla:6]},
  abstract      = {The application of Intelligent Transportation System (ITS) is very important in developing societies nowadays. Vehicle monitoring is one of the primary tasks of ITS, where vehicles are classified by lanes for traffic management, especially in case of a mixed flow of motorcycles and other automobiles in the transport system of Vietnam. This paper proposes a new approach in vehicle classification, which is based on evaluation of the direction angle of the first primary axis of each coming vehicle detected in the captured video sequence and map into the predetermined database to mark it as motorcycle or automobiles instead of consideration of vehicle size. The experimental results show that the classification performance is promising, especially for motorcycles and cars, and therefore is applicable in detection of vehicle penalties moving in wrong lanes.},
  doi           = {10.1109/CCE.2016.7562663},
  keywords      = {automobiles;image classification;image sequences;intelligent transportation systems;motorcycles;object detection;video signal processing;ITS;automobile;direction angle estimation;intelligent transportation system application;mixed traffic flow;motorcycle mixed flow;traffic management;vehicle classification;vehicle detection;vehicle monitoring;video sequence;Automobiles;Cameras;Motorcycles;Surveillance;Video sequences;ITS;car;motorcycle;vehicle classification},
}

@Article{Li2014,
  author        = {S. Li and H. Yu and J. Zhang and K. Yang and R. Bin},
  title         = {Video-based traffic data collection system for multiple vehicle types},
  journal       = {IET Intelligent Transport Systems},
  year          = {2014},
  volume        = {8},
  number        = {2},
  pages         = {164-174},
  month         = {March},
  issn          = {1751-956X},
  __markedentry = {[Barancsuk Lilla:6]},
  abstract      = {Traffic data of multiple vehicle types are important for pavement design, traffic operations and traffic control. A new video-based traffic data collection system for multiple vehicle types is developed. By tracking and classifying every passing vehicle under mixed traffic conditions, the type and speed of every passing vehicle are recognised. Finally, the flows and mean speeds of multiple vehicle types are output. A colour image-based adaptive background subtraction is proposed to obtain more accurate vehicle objects, and a series of processes like shadow removal and setting road detection region are used to improve the system robustness. In order to improve the accuracy of vehicle counting, the cross-lane vehicles are detected and repeated counting for one vehicle is avoided. In order to reduce the classification errors, the space ratio of the blob and data fusion are used to reduce the classification errors caused by vehicle occlusions. This system was tested under four different weather conditions. The accuracy of vehicle counting was 97.4% and the error of vehicle classification was 8.3%. The correlation coefficient of speeds detected by this system and radar gun was 0.898 and the mean absolute error of speed detection by this system was only 2.3 km/h.},
  doi           = {10.1049/iet-its.2012.0099},
  keywords      = {image classification;image colour analysis;image fusion;object tracking;road traffic;traffic engineering computing;video signal processing;blob space ratio;classification error reduction;colour image-based adaptive background subtraction;cross-lane vehicle;data fusion;mean absolute error;mixed traffic condition;passing vehicle classification;passing vehicle speed;passing vehicle tracking;passing vehicle type;pavement design;road detection setting;shadow removal process;speed detection;traffic control;traffic operation;vehicle counting;vehicle occlusion;vehicle type;video-based traffic data collection system},
}

@InProceedings{Li2016,
  author        = {Xiaofei Li and F. Flohr and Yue Yang and Hui Xiong and M. Braun and S. Pan and Keqiang Li and D. M. Gavrila},
  title         = {A new benchmark for vision-based cyclist detection},
  booktitle     = {2016 IEEE Intelligent Vehicles Symposium (IV)},
  year          = {2016},
  pages         = {1028-1033},
  month         = {June},
  __markedentry = {[Barancsuk Lilla:6]},
  abstract      = {Significant progress has been achieved over the past decade on vision-based pedestrian detection; this has led to active pedestrian safety systems being deployed in most mid- to high-range cars on the market. Comparatively little effort has been spent on vision-based cyclist detection, especially when it concerns quantitative performance analysis on large datasets. We present a large-scale experimental study on cyclist detection where we examine the currently most promising object detection methods; we consider Aggregated Channel Features, Deformable Part Models and Region-based Convolutional Neural Networks. We also introduce a new method called Stereo-Proposal based Fast R-CNN (SP-FRCN) to detect cyclists based on stereo proposals and Fast R-CNN (FRCN) framework. Experiments are performed on a dataset containing 22161 annotated cyclist instances in over 30000 images, recorded from a moving vehicle in the urban traffic of Beijing. Results indicate that all the three solution families can reach top performance around 0.89 average precision on the easy case, but the performance drops gradually with the difficulty increasing. The dataset including rich annotations, stereo images and evaluation scripts (termed â€śTsinghua-Daimler Cyclist Benchmarkâ€ť) is made public to the scientific community, to serve as a common point of reference for future research.},
  doi           = {10.1109/IVS.2016.7535515},
  keywords      = {neural nets;object detection;pedestrians;road safety;stereo image processing;Fast R-CNN;SP-FRCN;active pedestrian safety systems;aggregated channel features;deformable part models;object detection;region-based convolutional neural networks;stereo images;stereo-proposal based fast R-CNN;vision-based cyclist detection;vision-based pedestrian detection;Benchmark testing;Detectors;Feature extraction;Object detection;Proposals;Roads;Training},
}

@Article{Mithun2012,
  author        = {N. C. Mithun and N. U. Rashid and S. M. M. Rahman},
  title         = {Detection and Classification of Vehicles From Video Using Multiple Time-Spatial Images},
  journal       = {IEEE Transactions on Intelligent Transportation Systems},
  year          = {2012},
  volume        = {13},
  number        = {3},
  pages         = {1215-1225},
  month         = {Sept},
  issn          = {1524-9050},
  __markedentry = {[Barancsuk Lilla:6]},
  abstract      = {Detection and classification of vehicles are two of the most challenging tasks of a video-based intelligent transportation system. Traditional detection and classification methods are computationally highly expensive and become unsuccessful in many cases such as occlusion among the vehicles and when differences between pixel intensities of vehicles and backgrounds are small. In this paper, a novel detection and classification method is proposed using multiple time-spatial images (TSIs), each obtained from a virtual detection line on the frames of a video. Such a use of multiple TSIs provides the opportunity to identify the latent occlusions among the vehicles and to reduce the dependencies of the pixel intensities between the still and moving objects to increase the accuracy of detection performance as well as to achieve an improved classification performance. In order to identify the class of a particular vehicle, a two-step k nearest neighborhood classification scheme is proposed by utilizing the shape-based, shape-invariant, and texture-based features of the segmented regions corresponding to the vehicle appeared in appropriate frames that are determined from the TSIs of the video. Extensive experimentations are carried out in vehicular traffics of varying environments to evaluate the detection and classification performance of the proposed method, as compared with the existing methods. Experimental results demonstrate that the proposed method provides a significant improvement in counting and classifying the vehicles in terms of accuracy and robustness alongside a substantial reduction of execution time, as compared with that of the other methods.},
  doi           = {10.1109/TITS.2012.2186128},
  keywords      = {automated highways;feature extraction;image classification;image segmentation;image texture;learning (artificial intelligence);object detection;road traffic;traffic engineering computing;video signal processing;classification performance;detection performance;occlusion;segmented region;shape-based feature;shape-invariant feature;texture-based feature;time-spatial image;two-step k nearest neighborhood classification scheme;vehicle classification;vehicle detection;vehicle pixel intensity;vehicular traffic;video frame;video-based intelligent transportation system;virtual detection line;Algorithm design and analysis;Classification algorithms;Detection algorithms;Feature extraction;Image classification;Vehicles;Detection and classification of vehicles;time-spatial image (TSI);virtual detection line (VDL)},
}

@Article{Morris2012,
  author        = {B. T. Morris and C. Tran and G. Scora and M. M. Trivedi and M. J. Barth},
  title         = {Real-Time Video-Based Traffic Measurement and Visualization System for Energy/Emissions},
  journal       = {IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS},
  year          = {2012},
  volume        = {13},
  number        = {4},
  pages         = {1667â€“1677},
  month         = {December},
  __markedentry = {[Barancsuk Lilla:6]},
  owner         = {BarancsukLilla},
  timestamp     = {2016.10.03},
}

@InProceedings{Oeztuerk2016,
  author        = {Ş Öztürk and B. ?nan and Y. Artan},
  title         = {Color based vehicle classification in surveillance videos},
  booktitle     = {2016 24th Signal Processing and Communication Application Conference (SIU)},
  year          = {2016},
  pages         = {1661-1664},
  month         = {May},
  __markedentry = {[Barancsuk Lilla:6]},
  abstract      = {In this paper, we develop a novel method for vehicle classification using color images from surveillance videos. Proposed approach utilizes deformable part based (DPM) object detectors to localize different parts of the vehicle such as license plates and headlights. Next, we extract color features around the detected regions to infer the color of the vehicle. Color descriptors are used to detect the color of the relevant parts of the vehicle instead of using raw RGB data. If the classifier output matches target color, system triggers a kernelized correlation filter (KCF) based tracker on that vehicle to follow its trajectory. Experiments are conducted using more than 1200 real-world images that have a large variation in reflection, shadow and other illumination effects.},
  doi           = {10.1109/SIU.2016.7496076},
  keywords      = {correlation methods;filtering theory;image classification;image colour analysis;traffic engineering computing;vehicles;video surveillance;DPM object detectors;KCF based tracker;color descriptors;color images;deformable part;headlights;illumination effects;kernelized correlation filter;license plates;raw RGB data;surveillance videos;vehicle classification;Conferences;Deformable models;Histograms;Image color analysis;Pattern recognition;Vehicles;Videos;Classification;car tracking;color;target detection},
}

@InProceedings{Ping2015,
  author        = {Z. Ping and L. Qian and Z. Siyang},
  title         = {Adaptive background updating algorithm for traffic congestion detection based on Kalman filtering and inter-frame centroid distanc},
  booktitle     = {2015 IEEE Advanced Information Technology, Electronic and Automation Control Conference (IAEAC)},
  year          = {2015},
  pages         = {891-895},
  month         = {Dec},
  __markedentry = {[Barancsuk Lilla:6]},
  abstract      = {As current video based vehicle detection algorithms can not detect the traffic congestion accurately, this paper presents a new adaptive background updating algorithm based on Kalman filtering and inter-frame centroid distance. Firstly, a Gauss mixture background model is set up to extract the moving vehicles. Then, with Kalman filtering method, the moving vehicles are tracked to identify their motion states. This method predicts the centroid position of the next frame vehicles. The Euclidean distance of the centroids of the adjacent frames vehicles are counted and the appropriate threshold is set up to realize the identification and the mark of stationary vehicles in the video. This improved background updating algorithms can better judge the traffic congestion, and it lays a foundation for improving the accuracy rate of the detection of traffic flow. The proposed algorithm has been tested for multiple traffic videos. The results show that the algorithm is of good real-time ability, environmental adaptability and accuracy.},
  doi           = {10.1109/IAEAC.2015.7428685},
  keywords      = {Gaussian processes;Kalman filters;mixture models;object detection;object tracking;road traffic;road vehicles;traffic engineering computing;Euclidean distance;Gauss mixture background model;Kalman filtering method;adaptive background updating algorithm;adjacent frames vehicles;appropriate threshold;background updating algorithms;centroid position;environmental adaptability;interframe centroid distance;motion states;moving vehicles;multiple traffic videos;next frame vehicles;stationary vehicles;traffic congestion detection;traffic flow;video based vehicle detection algorithms;Buildings;Decision support systems;Kalman filters;Mixture models;Euclidean distance;Gaussian mixture model;Kalman filtering method;traffic congestion},
}

@InProceedings{Ren2013,
  author        = {J. Ren and L. Xin and Y. Chen and D. Yang},
  title         = {High-efficient detection of traffic parameters by using two foreground temporal-spatial images},
  booktitle     = {16th International IEEE Conference on Intelligent Transportation Systems (ITSC 2013)},
  year          = {2013},
  pages         = {1965-1970},
  month         = {Oct},
  __markedentry = {[Barancsuk Lilla:6]},
  abstract      = {Real-time detection of vehicular volume, mean speed and vehicle type has important significance, but the existing video-based detection methods are not satisfactory at processing speed and accuracy. This paper proposes a high-efficient method to detect all the three parameters from two foreground temporal-spatial images (TSIs) directly, which are obtained from two virtual detection lines (VDLs) in video frames. Such usage of the TSIs provides a feasible approach to solve the problems of vehicle occlusion, mean-speed estimation, and vehicle classification without using original frame images. Firstly, for improving the accuracy of detection, during generation of the foreground TSIs, we set a small-wide region of interest for each VDL and propose a local background subtraction method and an improved moving shadows elimination method to eliminate unwanted interferences. Then, in order to reduce the calculation complexity, during extraction of the parameters, we analyze the feasibility of vehicle classification direct from the foreground TSIs, and propose a method to extract shape-feature vector from the TSIs directly. The dependence on original frame images is minimized, so the pressing speed is improved obviously. Experimental results prove the feasibility and efficiency of the proposed method.},
  doi           = {10.1109/ITSC.2013.6728517},
  issn          = {2153-0009},
  keywords      = {feature extraction;image classification;image sequences;object detection;road traffic;spatiotemporal phenomena;traffic engineering computing;TSI;VDL;calculation complexity;foreground temporal-spatial images;high-efficient traffic parameter detection;improved moving shadow elimination method;interference elimination;local background subtraction method;mean-speed estimation;parameter extraction;real-time mean speed detection;real-time vehicle type detection;real-time vehicular volume detection;shape-feature vector extraction method;vehicle classification;vehicle occlusion;video frames;video-based detection methods;virtual detection lines;Accuracy;Cameras;Feature extraction;Roads;Shape;Support vector machine classification;Vehicles;Detection of traffic parameters;elimination of moving shadows;foreground temporal-spatial image;local background subtraction;vehicle classification},
}

@InProceedings{Saran2015,
  author        = {K. B. Saran and G. Sreelekha},
  title         = {Traffic video surveillance: Vehicle detection and classification},
  booktitle     = {2015 International Conference on Control Communication Computing India (ICCC)},
  year          = {2015},
  pages         = {516-521},
  month         = {Nov},
  __markedentry = {[Barancsuk Lilla:6]},
  abstract      = {Vehicle detection and classification is the most important and challenging stage of traffic surveillance using computer vision techniques. The videos captured using the closed circuit television (CCTV) cameras placed in roadsides or driveways are used for the surveillance. The surveillance system includes detection of moving vehicles, counting the number of vehicles and the classification of the detected vehicles. The main challenge of the computer vision technique is the real time applicability of the algorithms used. In this work a vehicle detection and classification algorithm which works in real time is proposed. The detection is carried out by the method of background subtraction where the background is modeled using the mixture of Gaussians and the detected vehicles are classified using the Artificial Neural Network (ANN) with a new set of features, Histograms of Oriented Gradients (HOG) and geometric measures of the vehicles. Experimental results shows that the proposed method with the new combination of features as training parameters for ANN give better result as compared to other popular algorithms.},
  doi           = {10.1109/ICCC.2015.7432948},
  keywords      = {closed circuit television;image classification;image motion analysis;learning (artificial intelligence);neural nets;object detection;road traffic;road vehicles;traffic engineering computing;video cameras;video surveillance;ANN;CCTV cameras;Gaussians mixture;HOG;artificial neural network;background subtraction method;closed circuit television cameras;computer vision techniques;driveways;geometric measures;histogram of oriented gradients;moving vehicle classification algorithm;moving vehicle detection algorithm;roadsides;traffic video surveillance;training parameters;video capture;Cameras;Computer vision;Real-time systems;Vehicle detection;Vehicles;Video surveillance},
}

@InProceedings{Saunier2006,
  author        = {N. Saunier and T. Sayed},
  title         = {A Feature-based Tracking Algorithm for Vehicles in Intersections},
  booktitle     = {Proceedings of the The 3rd Canadian Conference on Computer and Robot Vision},
  year          = {2006},
  series        = {CRV '06},
  pages         = {59--},
  address       = {Washington, DC, USA},
  publisher     = {IEEE Computer Society},
  __markedentry = {[Barancsuk Lilla:6]},
  acmid         = {1135594},
  doi           = {10.1109/CRV.2006.3},
  isbn          = {0-7695-2542-3},
  url           = {http://dx.doi.org/10.1109/CRV.2006.3},
}

@InProceedings{Tian2011,
  author        = {B. Tian and Q. Yao and Y. Gu and K. Wang and Y. Li},
  title         = {Video processing techniques for traffic flow monitoring: A survey},
  booktitle     = {2011 14th International IEEE Conference on Intelligent Transportation Systems (ITSC)},
  year          = {2011},
  pages         = {1103-1108},
  month         = {Oct},
  __markedentry = {[Barancsuk Lilla:6]},
  abstract      = {Video-based traffic flow monitoring is a fast emerging field based on the continuous development of computer vision. A survey of the state-of-the-art video processing techniques in traffic flow monitoring is presented in this paper. Firstly, vehicle detection is the first step of video processing and detection methods are classified into background modeling based methods and non-background modeling based methods. In particular, nighttime detection is more challenging due to bad illumination and sensitivity to light. Then tracking techniques, including 3D model-based, region-based, active contour-based and feature-based tracking, are presented. A variety of algorithms including MeanShift algorithm, Kalman Filter and Particle Filter are applied in tracking process. In addition, shadow detection and vehicles occlusion bring much trouble into vehicle detection, tracking and so on. Based on the aforementioned video processing techniques, discussion on behavior understanding including traffic incident detection is carried out. Finally, key challenges in traffic flow monitoring are discussed.},
  doi           = {10.1109/ITSC.2011.6083125},
  issn          = {2153-0009},
  keywords      = {Kalman filters;computer vision;particle filtering (numerical methods);road traffic;video signal processing;Kalman filter;active contour-based tracking;background modeling based methods;computer vision;feature-based tracking;illumination;nighttime detection;particle filter;shadow detection;traffic flow monitoring;vehicle detection;video processing techniques;video-based traffic flow monitoring;Computational modeling;Image edge detection;Monitoring;Tracking;Trajectory;Vehicle detection;Vehicles},
}

@InProceedings{Tursun2013,
  author        = {M. Tursun and G. Amrulla},
  title         = {A video based real-time vehicle counting system using optimized virtual loop method},
  booktitle     = {Systems, Signal Processing and their Applications (WoSSPA), 2013 8th International Workshop on},
  year          = {2013},
  pages         = {75-78},
  month         = {May},
  __markedentry = {[Barancsuk Lilla:6]},
  abstract      = {This paper describes a computer vision system to detect and count moving vehicles on roads. The system uses a real-time traffic video surveillance camera mounted over roads and computes the total number of vehicles which passed the road. Moving vehicle image is extracted using `double difference image `algorithm and counting is accomplished by tracking vehicle movements within a tracking zone, called virtual loop. The system was tested on a video surveillance record file of a road that has a medium-level traffic volume.},
  doi           = {10.1109/WoSSPA.2013.6602339},
  keywords      = {computer vision;feature extraction;object detection;video cameras;video surveillance;computer vision system;double-difference image algorithm;medium-level traffic volume;moving vehicle image extraction;optimized virtual loop method;real-time traffic video surveillance camera;video surveillance record file;video-based real-time vehicle counting system;Cameras;Computer vision;Detectors;Roads;Shape;Streaming media;Vehicles},
}

@InProceedings{Van2014,
  author        = {C. N. Van and C. N. Ngoc},
  title         = {Vehicle Classification in Video Based on Shape Analysis},
  booktitle     = {Modelling Symposium (EMS), 2014 European},
  year          = {2014},
  pages         = {151-157},
  month         = {Oct},
  __markedentry = {[Barancsuk Lilla:6]},
  abstract      = {This paper aims at presenting some methods of representing image's features that help detect and classify vehicles from video. Proposed methods include: Method of representing shape, contour of vehicle or block of vehicle that can be classified. Paramaters of the Image's length in combination with parmaters of visual length of object that can used to classify object type or separate object. Use genaral deformable model of vehicle for allowing to be completely or partially occluding in the image. Apply some proposed methods of representing vehicle for vehicle recognition and classification system in traffic video. This paper also proposes a general working frame for the video - based traffic density detection and vehicle classification system in observation region. System was experimentally installed and obtained good results about the level of accuracy.},
  doi           = {10.1109/EMS.2014.27},
  keywords      = {automobiles;image classification;image representation;image sequences;road traffic;traffic engineering computing;video signal processing;completely occluded image;genaral deformable model;image feature representation;image length;object type classification;observation region;partially occluded image;shape analysis;shape representation;traffic video;vehicle block;vehicle classification system;vehicle contour;vehicle detection;vehicle recognition system;vehicle representation;video-based traffic density detection system;visual length;Cameras;Deformable models;Feature extraction;Motorcycles;Roads;Shape;Car Counting;Contour Analysis;Optical Flow;Shape Detection},
}

@Article{Yang2013,
  author        = {M. T. Yang and R. K. Jhang and J. S. Hou},
  title         = {Traffic flow estimation and vehicle-type classification using vision-based spatial-temporal profile analysis},
  journal       = {IET Computer Vision},
  year          = {2013},
  volume        = {7},
  number        = {5},
  pages         = {394-404},
  month         = {October},
  issn          = {1751-9632},
  __markedentry = {[Barancsuk Lilla:6]},
  abstract      = {Vision-based traffic surveillance plays an important role in traffic management. However, outdoor illuminations, the cast shadows and vehicle variations often create problems for video analysis and processing. Thus, the authors propose a real-time cost-effective traffic monitoring system that can reliably perform traffic flow estimation and vehicle classification at the same time. First, the foreground is extracted using a pixel-wise weighting list that models the dynamic background. Shadows are discriminated utilising colour and edge invariants. Second, the foreground on a specified check-line is then collected over time to form a spatial-temporal profile image. Third, the traffic flow is estimated by counting the number of connected components in the profile image. Finally, the vehicle type is classified according to the size of the foreground mask region. In addition, several traffic measures, including traffic velocity, flow, occupancy and density, are estimated based on the analysis of the segmentation. The availability and reliability of these traffic measures provides critical information for public transportation monitoring and intelligent traffic control. Since the proposed method only process a small area close to the check-line to collect the spatial-temporal profile for analysis, the complete system is much more efficient than existing visual traffic flow estimation methods.},
  doi           = {10.1049/iet-cvi.2012.0185},
  keywords      = {edge detection;feature extraction;image classification;image colour analysis;image segmentation;road traffic;road vehicles;traffic engineering computing;transportation;video signal processing;cast shadow;colour invariant;dynamic background;edge invariant;foreground extraction;foreground mask region;intelligent traffic control;outdoor illumination;pixel-wise weighting list;public transportation monitoring;segmentation analysis;spatial-temporal profile image;traffic flow estimation;traffic management;traffic monitoring system;vehicle classification;vehicle type classification;vehicle-type classification;video analysis;vision-based spatial-temporal profile analysis;vision-based traffic surveillance},
}

@Article{Zhou2007,
  author        = {J. Zhou and D. Gao and D. Zhang},
  title         = {Moving Vehicle Detection for Automatic Traffic Monitoring},
  journal       = {IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY},
  year          = {2007},
  volume        = {56},
  number        = {1},
  pages         = {51â€“59},
  month         = {January},
  __markedentry = {[Barancsuk Lilla:6]},
  owner         = {BarancsukLilla},
  timestamp     = {2016.10.04},
}

@InProceedings{Zhou2014,
  author        = {T. Zhou and Y. Yan},
  title         = {Video target tracking based on mean shift algorithm with Kalman filter},
  booktitle     = {2014 10th International Conference on Natural Computation (ICNC)},
  year          = {2014},
  pages         = {980-984},
  month         = {Aug},
  __markedentry = {[Barancsuk Lilla:6]},
  abstract      = {The mean shift based object tracking algorithm has been successfully applied in visual tracking due to its real-timeliness and robustness. The key module is mean shift iterations and it eventually converges to the object position in the current frame. In this paper, we prove its efficiency by experiments and discuss its weakness. An improved algorithm combining Kalman filter with mean shift is presented, aiming at basic mean shift algorithm fails to track fast-moving object.},
  doi           = {10.1109/ICNC.2014.6975973},
  issn          = {2157-9555},
  keywords      = {Kalman filters;iterative methods;object tracking;target tracking;video signal processing;Kalman filter;mean shift based object tracking algorithm;mean shift iterations;video target tracking algorithm;visual tracking;Algorithm design and analysis;Computational modeling;Kalman filters;Kernel;Target tracking;Visualization;Kalman Filter;Mean Shift;Target Tracking},
}

@Article{Zivkovic2004,
  author        = {Zoran Zivkovic},
  title         = {Improved adaptive gaussian mixture model for background subtraction},
  journal       = {Pattern Recognition},
  year          = {2004},
  volume        = {2},
  pages         = {28â€“31},
  __markedentry = {[Barancsuk Lilla:6]},
  owner         = {BarancsukLilla},
  timestamp     = {2016.10.03},
}

@Article{Zivkovic2006,
  author        = {Zoran Zivkovic and Ferdinand van der Heijden},
  title         = {Efficient adaptive density estimation per image pixel for the task of background subtraction},
  journal       = {Pattern recognition letters},
  year          = {2006},
  volume        = {27},
  number        = {7},
  pages         = {773â€“780},
  __markedentry = {[Barancsuk Lilla:6]},
  owner         = {BarancsukLilla},
  timestamp     = {2016.10.03},
}

@Electronic{AzoSensor,
  organization  = {AZO Sensors},
  url           = {http://www.azosensors.com/article.aspx?ArticleID=95, Viewed: 2016. 10. 06.},
  __markedentry = {[Barancsuk Lilla:6]},
  owner         = {BarancsukLilla},
  timestamp     = {2016.10.06},
}

@Electronic{DeepBlue,
  organization  = {trafficnow},
  note          = {radar based},
  url           = {http://deepbluesensor.com/, Letöltve: 2016. 10. 03.},
  __markedentry = {[Barancsuk Lilla:6]},
  owner         = {BarancsukLilla},
  timestamp     = {2016.10.03},
}

@Electronic{Diamond,
  organization  = {Diamond Traffic Products},
  url           = {http://diamondtraffic.com/technicaldescription/124, Letöltve: 2016. 10. 06.},
  __markedentry = {[Barancsuk Lilla:6]},
  owner         = {BarancsukLilla},
  timestamp     = {2016.10.06},
}

@Electronic{MagyarKozut,
  organization  = {Magyar Közút},
  note          = {kozuthalozat},
  url           = {http://internet.kozut.hu/Lapok/forgalomszamlalas.aspx, Letöltve: 2016. 10. 06.},
  __markedentry = {[Barancsuk Lilla:6]},
  owner         = {BarancsukLilla},
  timestamp     = {2016.10.06},
}

@Electronic{SICK,
  language      = {English},
  organization  = {SICK - Sensor Intelligence},
  note          = {laser},
  url           = {http://www.roadtraffic-technology.com/products/vps-pro, : 2016. 10. 03.SICK},
  __markedentry = {[Barancsuk Lilla:6]},
  owner         = {BarancsukLilla},
  timestamp     = {2016.10.03},
}

@Electronic{Te,
  organization  = {TE CONNECTIVITY},
  note          = {piezo-weight},
  url           = {http://www.te.com/usa-en/products/sensors/traffic-sensors.html?tab=pgp-story, : 2016. 10. 03.},
  __markedentry = {[Barancsuk Lilla:6]},
  owner         = {BarancsukLilla},
  timestamp     = {2016.10.03},
}

@Electronic{LaSemaforica,
  organization  = {La Semaforica},
  note          = {video},
  url           = {http://www.lasemaforica.com/en/products/traffic-data-survey-systems/video-detectors/traficam, : 2016. 10. 03.},
  __markedentry = {[Barancsuk Lilla:6]},
  owner         = {BarancsukLilla},
  timestamp     = {2016.10.03},
}

@Electronic{Swarco,
  organization  = {SWARCO},
  note          = {inductive, video, infra red, radar},
  url           = {https://www.swarco.com/en/Products-Services/Traffic-Management/Urban-Traffic-Management/Traffic-Detectors, LetĂ¶ltve: 2016. 10. 03.},
  __markedentry = {[Barancsuk Lilla:6]},
  owner         = {BarancsukLilla},
  timestamp     = {2016.10.03},
}

@Electronic{VideoSurveillance,
  organization  = {VideoSurveillanceVideo},
  note          = {video},
  url           = {https://www.videosurveillance.com/traffic.asp, : 2016. 10. 03.},
  __markedentry = {[Barancsuk Lilla:6]},
  owner         = {BarancsukLilla},
  timestamp     = {2016.10.03},
}

@InProceedings{4564937,
  author    = {A. Koutsia and T. Semertzidis and K. Dimitropoulos and N. Grammalidis and K. Georgouleas},
  title     = {Intelligent traffic monitoring and surveillance with multiple cameras},
  booktitle = {2008 International Workshop on Content-Based Multimedia Indexing},
  year      = {2008},
  pages     = {125-132},
  month     = {June},
  doi       = {10.1109/CBMI.2008.4564937},
  issn      = {1949-3983},
  keywords  = {automated highways;computerised monitoring;image fusion;image motion analysis;image sensors;traffic engineering computing;video surveillance;TRAVIS;aircraft parking;autonomous tracking units;data fusion;highways;image processing;intelligent traffic monitoring;intelligent traffic surveillance;multiple cameras;real-time vision system;remote traffic control centre;traffic visual monitoring;Air traffic control;Aircraft;Computerized monitoring;Image processing;Machine vision;Real time systems;Road transportation;Smart cameras;Surveillance;Telecommunication traffic},
}

@InProceedings{Gallego2009,
  author    = {N. Gallego and A. Mocholi and M. Menendez and R. Barrales},
  title     = {Traffic Monitoring: Improving Road Safety Using a Laser Scanner Sensor},
  booktitle = {2009 Electronics, Robotics and Automotive Mechanics Conference (CERMA)},
  year      = {2009},
  pages     = {281-286},
  month     = {Sept},
  doi       = {10.1109/CERMA.2009.11},
  keywords  = {legislation;monitoring;optical scanners;optical sensors;road safety;road traffic;road vehicles;traffic engineering computing;ITS application;TCC;bus;environmental noise;environmental pollution;information society;intelligent transport system;laser scanner sensor;oil dependency;passenger car;powered two-wheeler;road safety legislation;strategic traffic management;traffic control centre;traffic monitoring;traffic policy;truck;urban area;van;vehicle classification;vehicle detection system;Environmental economics;Environmental management;Intelligent sensors;Intelligent systems;Laser noise;Monitoring;Oil pollution;Petroleum;Power generation economics;Road safety;Intelligent Transport Systems (ITS);Intelligent sensors;Laser Scanner;detection system;traffic parameters},
}

@InProceedings{Thiruverahan2015,
  author    = {N. Thiruverahan and P. Ravi and S. G. Jacob},
  title     = {Adaptive traffic management and dynamic optimal redirection using particle swarm optimization},
  booktitle = {2015 International Conference on Applied and Theoretical Computing and Communication Technology (iCATccT)},
  year      = {2015},
  pages     = {477-480},
  month     = {Oct},
  doi       = {10.1109/ICATCCT.2015.7456931},
  keywords  = {adaptive control;finite state machines;particle swarm optimisation;piezoelectric transducers;road traffic control;scheduling;adaptive traffic control system;adaptive traffic management;centralized control unit;dynamic optimal redirection;fair scheduling;particle swarm optimization;piezoelectric strips;road;round-robin scheduling;state machine;traffic intensity information;traffic movement scheduling;Communications technology;Conferences;fair scheduling;machine learning;optimal route;particle swarm optimization;traffic management},
}

@Article{Rivas2017,
  author   = {J. Rivas and R. Wunderlich and S. J. Heinen},
  title    = {Road Vibrations as a Source to Detect the Presence and Speed of Vehicles},
  journal  = {IEEE Sensors Journal},
  year     = {2017},
  volume   = {17},
  number   = {2},
  pages    = {377-385},
  month    = {Jan},
  issn     = {1530-437X},
  doi      = {10.1109/JSEN.2016.2628858},
  keywords = {accelerometers;microsensors;piezoelectric transducers;velocity measurement;2D sensor network;MEMS accelerometer;energy harvesting;piezoelectric acceleration sensor;piezoelectric film;pollution emission;road vibration;smart street;vehicle speed detection;vehicular traffic flow monitoring;Frequency measurement;Micromechanical devices;Monitoring;Roads;Sensors;Vehicles;Vibrations;Piezoelectric;acceleration;asphalt;classification;matlab;sensor;sensor network;speed;travel direction;vehicles;velocity;vibrations},
}

@Article{Morris2012a,
  author   = {B. T. Morris and C. Tran and G. Scora and M. M. Trivedi and M. J. Barth},
  title    = {Real-Time Video-Based Traffic Measurement and Visualization System for Energy/Emissions},
  journal  = {IEEE Transactions on Intelligent Transportation Systems},
  year     = {2012},
  volume   = {13},
  number   = {4},
  pages    = {1667-1678},
  month    = {Dec},
  issn     = {1524-9050},
  doi      = {10.1109/TITS.2012.2208222},
  keywords = {air pollution control;automated highways;computer vision;fuel economy;object recognition;road traffic;traffic engineering computing;video surveillance;wireless sensor networks;CO emission;CO2 emission;CalSentry system;HC emission;NOx emission;computer vision;congestion traffic management system;environmental factor;mobile sensor data processing;roadway network;roadway state monitoring;static data processing;traffic fuel economy;vehicle recognition;vehicle specific power-based emission model;vehicle specific power-based energy model;vehicle trajectory;video-based traffic measurement;visualization system;Monitoring;Pollution measurement;Real-time systems;Road transportation;Traffic control;Visualization;Intelligent transportation systems (ITS);real-time energy/emissions estimation;traffic measurement and management;vehicle specific power energy and modal emissions;visual tracking and classification},
}

@Article{Hussain1995,
  author   = {T. M. Hussain and A. M. Baig and T. N. Saadawi and S. A. Ahmed},
  title    = {Infrared pyroelectric sensor for detection of vehicular traffic using digital signal processing techniques},
  journal  = {IEEE Transactions on Vehicular Technology},
  year     = {1995},
  volume   = {44},
  number   = {3},
  pages    = {683-689},
  month    = {Aug},
  issn     = {0018-9545},
  doi      = {10.1109/25.406637},
  keywords = {correlation methods;infrared detectors;pyroelectric detectors;road traffic;signal processing;traffic control;correlation techniques;digital signal processing techniques;field tests;infrared pyroelectric sensor;laboratory tests;passive infrared system;road traffic monitoring;vehicular traffic detection;Computerized monitoring;Infrared detectors;Infrared sensors;Infrared surveillance;Laboratories;Pyroelectricity;Roads;Sensor systems;Signal processing;Vehicles},
}

@InProceedings{Ghazal2016,
  author    = {B. Ghazal and K. ElKhatib and K. Chahine and M. Kherfan},
  title     = {Smart traffic light control system},
  booktitle = {2016 Third International Conference on Electrical, Electronics, Computer Engineering and their Applications (EECEA)},
  year      = {2016},
  pages     = {140-145},
  month     = {April},
  doi       = {10.1109/EECEA.2016.7470780},
  keywords  = {automobiles;control system synthesis;infrared detectors;pedestrians;road accidents;road traffic control;synchronisation;traffic engineering computing;IR sensors;PIC microcontroller;accidents;adjacent traffic light systems;automobile flow control;automobile flow monitor;cars;congestion;emergency vehicles;multiple traffic light systems;pedestrian crossing;portable controller device;roads;smart traffic light control system;synchronization;traffic density;traffic jam;transportation routes;Automobiles;Radio frequency;Relays;Roads;IR sensor;Traffic light system;XBee wireless communication;microcontroller;traffic density},
}

@Article{Zhang2016,
  author   = {Y. Zhang and C. Zhao and Q. Zhang},
  title    = {Counting vehicles in urban traffic scenes using foreground time-spatial images},
  journal  = {IET Intelligent Transport Systems},
  year     = {2016},
  volume   = {11},
  number   = {2},
  pages    = {61-67},
  issn     = {1751-956X},
  doi      = {10.1049/iet-its.2016.0162},
  keywords = {intelligent transportation systems;natural scenes;road vehicles;traffic engineering computing;video signal processing;FTSI;confidence measurements;connected component convexity;foreground time-spatial images;self-adaptive sample consensus background model;urban traffic scenes;urban traffic videos;vehicles counting methods;video frames;virtual detection line},
}

@Article{Kamkar2016,
  author   = {S. Kamkar and R. Safabakhsh},
  title    = {Vehicle detection, counting and classification in various conditions},
  journal  = {IET Intelligent Transport Systems},
  year     = {2016},
  volume   = {10},
  number   = {6},
  pages    = {406-413},
  issn     = {1751-956X},
  doi      = {10.1049/iet-its.2015.0157},
  keywords = {feature extraction;image classification;image restoration;intelligent transportation systems;matrix algebra;object detection;video signal processing;video streaming;active basis model;bounding box;camera vibration;feature extraction;grey-level co-occurrence matrix;image blurring;intelligent transportation systems;lighting conditions;random forest;reflection symmetry;time-spatial image;traffic monitoring systems;vehicle classification;vehicle counting;vehicle detection method;vehicle length;video streams;weather conditions},
}

@InProceedings{Kryjak2014,
  author    = {T. Kryjak and M. Komorkiewicz and M. Gorgon},
  title     = {Hardware-software implementation of vehicle detection and counting using virtual detection lines},
  booktitle = {Proceedings of the 2014 Conference on Design and Architectures for Signal and Image Processing},
  year      = {2014},
  pages     = {1-8},
  month     = {Oct},
  doi       = {10.1109/DASIP.2014.7115618},
  keywords  = {edge detection;image colour analysis;image sequences;object detection;road vehicles;traffic engineering computing;video signal processing;ITS;VDL;Zynq platform;census transform;hardware description language;hardware software implementation;hardware software system;horizontal edges;intelligent transportation system;smart camera;time spatial image;vehicle detection;virtual detection lines;Cameras;Field programmable gate arrays;Histograms;Image edge detection;Roads;Vehicle detection;Vehicles},
}

@InProceedings{Li2008,
  author    = {L. Li and L. Chen and X. Huang and J. Huang},
  title     = {A Traffic Congestion Estimation Approach from Video Using Time-Spatial Imagery},
  booktitle = {2008 First International Conference on Intelligent Networks and Intelligent Systems},
  year      = {2008},
  pages     = {465-469},
  month     = {Nov},
  doi       = {10.1109/ICINIS.2008.182},
  keywords  = {road traffic;traffic engineering computing;video signal processing;TV cameras;natural open world scene;road status;time-spatial imagery;traffic congestion detection;traffic congestion estimation;video signal processing;Cameras;Character generation;Communication system traffic control;Intelligent networks;Intelligent systems;Road vehicles;Robustness;Surveillance;Traffic control;Vehicle detection;Canny edge detection;Hough line detection;time-spatial image;traffic congestion estimation},
}

@InProceedings{Rashid2010,
  author    = {N. U. Rashid and N. C. Mithun and B. R. Joy and S. M. M. Rahman},
  title     = {Detection and classification of vehicles from a video using time-spatial image},
  booktitle = {International Conference on Electrical Computer Engineering (ICECE 2010)},
  year      = {2010},
  pages     = {502-505},
  month     = {Dec},
  doi       = {10.1109/ICELCE.2010.5700739},
  keywords  = {feature extraction;image classification;image segmentation;image sequences;object detection;traffic engineering computing;vehicles;video signal processing;conventional background subtraction-based methods;feature extraction;feature-based classification scheme;image segmentation;pixel intensities;time-spatial image;vehicle classification;vehicle detection;video sequences;video-based intelligent transportation system;virtual line;Algorithm design and analysis;Classification algorithms;Feature extraction;Image edge detection;Pixel;Vehicles;Video sequences;Detection and classification of vehicles;Time-spatial images;Video surveillance},
}

@Article{Mithun2012a,
  author   = {N. C. Mithun and N. U. Rashid and S. M. M. Rahman},
  title    = {Detection and Classification of Vehicles From Video Using Multiple Time-Spatial Images},
  journal  = {IEEE Transactions on Intelligent Transportation Systems},
  year     = {2012},
  volume   = {13},
  number   = {3},
  pages    = {1215-1225},
  month    = {Sept},
  issn     = {1524-9050},
  doi      = {10.1109/TITS.2012.2186128},
  keywords = {automated highways;feature extraction;image classification;image segmentation;image texture;learning (artificial intelligence);object detection;road traffic;traffic engineering computing;video signal processing;classification performance;detection performance;occlusion;segmented region;shape-based feature;shape-invariant feature;texture-based feature;time-spatial image;two-step k nearest neighborhood classification scheme;vehicle classification;vehicle detection;vehicle pixel intensity;vehicular traffic;video frame;video-based intelligent transportation system;virtual detection line;Algorithm design and analysis;Classification algorithms;Detection algorithms;Feature extraction;Image classification;Vehicles;Detection and classification of vehicles;time-spatial image (TSI);virtual detection line (VDL)},
}

@Comment{jabref-meta: databaseType:bibtex;}
