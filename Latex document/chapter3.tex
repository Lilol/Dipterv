%----------------------------------------------------------------------------
\chapter{The core software}\label{chap:Core software}
%----------------------------------------------------------------------------
This chapter presents the structure and operation of the core software system of the Traffic Sensor, in particular with regard to the effective image processing algorithms and software development paradigms that facilitate the real-time operation of the system.

The core software is responsible for the processing of the video stream itself.
It identifies the vehicles, classifies them and estimates parameters, including speed and following distance.
 
Besides that, the software supports other supplementary features, like the evaluation of the system's precision and correction of the calibration rectangle.
Although these features are implemented architecturally in the same software unit, the supplementary elements are discussed in chapter \ref{sec:SupplementarySoftware}., since they do not strictly belong to the core of the Traffic Sensor.
%----------------------------------------------------------------------------
\section{Software architecture}
%----------------------------------------------------------------------------
In this section the architectural parts of the framework, including the data storage technique, the processing method and the parallel operation of the Traffic Sensor core software are discussed.

\subsection{Media storage}
Timeline images can either be extracted from transformed versions of the original frame, or created from the latter calculated data.

\subsubsection{Timeline images}
\begin{enumerate}
	\item \textbf{Original Timeline}
	\item \textbf{MOG2 Timeline} 
	\item \textbf{Motionless Timeline}
	\item \textbf{Size, Speed, Following Distance Timeline: Parameter Timelines}
\end{enumerate}

\subsubsection{Frame-strips}
\begin{enumerate}
	\item \textbf{Raw Frame-strip}
	\item \textbf{Original Frame-strip}
	\item \textbf{MOG2 Frame-strip}
\end{enumerate}

\section{Plug-in architecture, Processors}
Processors

\section{Parallel processing}
%----------------------------------------------------------------------------
%----------------------------------------------------------------------------
\section{Processing steps}
%----------------------------------------------------------------------------
The processing of the video-stream consists of four main stages, as depicted in figure \ref{fig:processing_steps}.

\begin{figure}[bh]
	\centering
	\includesvg[width=\textwidth]{full_system_flowchart}
	%\scalebox{0.7}{	\LARGE\input{figures/full_system_flowchart.pdf_tex}}
	\caption{Steps of the processing of the video stream in the core software of the Traffic Sensor.\label{fig:processing_steps}}
\end{figure}

The first step is preprocessing, that is the remapping of each frame to achieve a standard form for effective processing.
Second, a background-subtraction and its post-corrections are applied to detect the moving vehicles precisely.
The third stage is data extraction, including vehicle detection, classification and parameter calculation.

The final step is system evaluation and testing. 
Since, this step is optional, and is only available when the system is tested during development, when the operation is not continuous, it is considered a supplementary software element, and is discussed in detail in chapter \ref{sec:SupplementarySoftware}.

In each processing stage a series of different timeline images and frame-strips are used.
%----------------------------------------------------------------------------
\subsection{Preprocessing}
%----------------------------------------------------------------------------
In the preprocessing stage a series of transforms are performed on each Raw Frame as seen on figure \ref{fig:transforms}.
As a result of remapping, a standard frame-scheme is created, that is independent of the video file format, camera type, placement and the sensor's environment.
This regular form has a pre-defined size, that is small enough to be processed in real-time, and is simple enough to be searched and measured on.
The result of the remapping of the Raw Frame is an Original Frame.
After s new column extracted from the Original Frame is added to the Original Timeline.

\begin{figure}[!h]
	\centering
	\includesvg[clean,width=\textwidth,pretex=\relsize{2}]{frame_transforms}
	%\scalebox{0.5}{\input{figures/frame_transforms.pdf_tex}}
	\caption{Preprocessing steps: the transformations performed on each frame before the background-subtraction and detection phase. The goal is to create an effective frame format for the following processing steps.\label{fig:transforms}}
\end{figure}

The first stage is perspective compensation.
At this point, the image, distorted by perspective transform, is remapped based on a calibration rectangle, so that lengths on the frame become independent from the distance of the camera.
The following vehicle-size calculation and classification strongly relies on the assumption that vehicle-lengths are not subject to perspective distortion, and are irrelative to their position on the frame.
The calibration rectangle is defined manually using the Project Configurator user interface (UI), that is detailed in chapter \ref{subs:ProjectConfigurator}.

The second stage is rotation of the frame, so that the tripwire becomes vertical.
As a result of the vertical tripwire, it is possible to measure in the dimension perpendicular to the tripwire simply by reading certain rows of the image.
This method, considering the image-storage technique of the OpenCV library, where frames are stored as an array of rows in the memory, speeds up the computation significantly, and simplifies searching on frames.

In the third level of preprocessing a frames are resized in order to decrease the number of pixels and the computation time, and increase the speed of processing.
The end-size of the frame can be specified through the configuration files, and is usually 320 by 240 pixels.
%----------------------------------------------------------------------------
\subsection{Background-subtraction}
%----------------------------------------------------------------------------
The second phase of processing is background-subtraction, that is the separation of moving objects form the static background on each frame.
This step creates the MOG2 Frame and Timeline as a result of background-subtraction.
The correction of the subtraction method use the Motionless Timeline.

The background is labelled using a GMM-based method, Mixture of Gaussians 2 (MOG2), that is available in the OpenCV library.
This method associates every pixel with a mixture of Gaussian intensity-distributions, and identifies each new data as background, if it is part of the constructed distribution-model, or as a moving object, if it falls out of the model's range.

The drawback of this technique is its complexity, that results in an increased computational cost and time.
Throughout the processing, background-subtraction is the most time-consuming of all of the outlined processing steps.
However, since the following steps rely on the accuracy of the result of the background-subtraction, the robustness and precision of the approach are essential.

Another drawback of background-modelling is that temporarily stopped objects that have been motionless for a while are missed, as they are being included into the background model.
The errors caused by this is corrected later on, by detecting immobile vehicles.

Although some approaches model only the tripwire's pixels as background--foreground, in our method background-subtraction takes place on the whole frame.
That is, because features, such as size and velocity are calculated by measuring objects' lengths on the MOG2 Frame, as further detailed in section \ref{chap:parameter_evaluation}.
After the background-subtraction, the result is corrected, in order to filter errors of the MOG2 output.

First, morphological opening followed by closing removes punctual noises, and smooths object contours on the MOG2 Frame.
As a result, the MOG2 Timeline is created.
After, stopped vehicles are detected, and saved to the Motionless Timeline.
As sudden light changes can cause an overreaction of MOG2, to prevent this, the MOG2 Frames are filtered.
If the count of a frame's white pixels exceeds a limit, the frame is cleared, and switched to background, to avoid false detections.
Although this process causes some vehicles to be missed, empirical analysis shows, that error rate is reasonably lower if the overreaction frames are filtered.
%----------------------------------------------------------------------------
\subsection{Data extraction}
%----------------------------------------------------------------------------
The required data is extracted after the MOG2 Timeline and Frame are created, and pixels of moving objects are identified.
First, blobs are selected with connected component analysis on the MOG2 Timeline image.

Afterwards the detection results are revised and corrected.
Some errors occur, because vehicles can fuse on the TI, if they cross the tripwire together. These merged objects are split based on their shapes, and saved as vehicles.

Also, errors may occur because of stopping vehicles, since their blobs slowly disappear and reappear on the MOG2 timeline.
Thus these vehicles have two separate blobs.
The split blobs are identified and recombined using the data of the Motionless Timeline. 

\begin{figure}[!h]
	\centering
	\includesvg[clean,width=\textwidth,pretex=\relsize{2}]{speed_size_following_distance}
	%\scalebox{0.5}{\input{figures/frame_transforms.pdf_tex}}
	\caption{.\label{fig:size_speed_following_distance}}
\end{figure}

After the vehicle detection, parameters are evaluated and stored on timeline images.
The following section discusses the estimation of these parameters.

\subsubsection{Parameter evaluation}\label{chap:parameter_evaluation}
The next phase of data extraction is parameter calculation.
At this point, the length, speed and following distance of the identified vehicles are estimated and stored in the form of a timeline image.
The length, speed and distance of the vehicles are measured on the MOG2 Frames, and projected back to the Parameter Timelines.

At each point on the Size Timeline the intensity value is calculated as follows:

\begin{displaymath}
 	\boldsymbol{TI_{\text{Size}}}(t,k) = 
	\begin{cases}
	l_{\text{L},t} + l_{\text{R},t}        & \quad \text{if } \boldsymbol{F_{\text{MOG2},t}} \text{ at the } k\text{th} \text{ point of the tripwire is occupied}\\
  	0		& \quad \text{if } \boldsymbol{F_{\text{MOG2},t}} \text{ at the } k\text{th} \text{ point of the tripwire is empty,}\\
	\end{cases}
\end{displaymath}

with $l_{\text{L},t}$, $l_{\text{R},t}$ being the perpendicular length of the left and right side of the object that occupies the tripwire.
The lengths are measured in pixels.

The intensities of the Speed Timeline are calculated with the lengths measured on 2 successive frames, as follows:

\begin{displaymath}
\boldsymbol{TI_{\text{Speed}}}(t_0,k) = 
\begin{cases}
\frac{\Delta r_{\text{L}} + \Delta r_{\text{R}}}{2}      & \quad \text{if } \boldsymbol{F_{\text{MOG2},t_0}} \text{ and } \boldsymbol{F_{\text{MOG2},t_1}} \text{ at the } k\text{th} \text{ point of the tripwire are occupied}\\
0		& \quad \text{otherwise,}\\
\end{cases}
\end{displaymath}
 
where $\Delta r_{\text{L}}$ and $\Delta r_{\text{R}}$ are the displacements of the left and right edge of the object, that occupies the tripwire.
The displacements are calculated with the left and right sides' lengths:
 
\begin{gather*}
\Delta r_{\text{L}} = \left( l_{\text{L},t_0} - l_{\text{L},t_1} \right),  \\
\Delta r_{\text{R}} = \left( l_{\text{R},t_1} - l_{\text{R},t_0}\right).
\end{gather*}

The intensity values of the Following Distance Timeline are calculated as follows:

\begin{displaymath}
\boldsymbol{TI_{\text{FD}}}(t,k) = 
\begin{cases}
d_{\text{L},t} + d_{\text{R},t} 		& \quad \text{if } \boldsymbol{F_{\text{MOG2},t}} \text{ at the } k\text{th} \text{ point of the tripwire is empty, and objects are deteted on each side of the} k \text{th point}\\
0		& \quad \text{otherwise},
\end{cases}
\end{displaymath}

with $d_{\text{L},t}$ and $d_{\text{R},t}$ being the left and right side distances of the nearest objects.
The distances are estimated by counting the empty pixels on each side of the tripwire.

\subsubsection{Classification}
%----------------------------------------------------------------------------
\begin{figure}[!h]
	\centering
	\begin{subfigure}[!h]{0.25\textwidth}
	\includesvg[width=\textwidth]{cars}
	\caption{Cars.}
	\end{subfigure}
	\quad
	\begin{subfigure}[!h]{0.25\textwidth}
	\includesvg[width=\textwidth]{trucks}
	\caption{Trucks.}
	\end{subfigure}
	\quad
	\begin{subfigure}[!h]{0.21\textwidth}
	\includesvg[width=\textwidth]{bicicle}
	\caption{Bicycles.}
	\end{subfigure}
	\hfill
	\begin{subfigure}[!h]{0.3\textwidth}
	\includesvg[width=\textwidth]{ped}
	\caption{Pedestrians.}
	\end{subfigure}
	\quad
	\begin{subfigure}[!h]{0.5\textwidth}
	\includesvg[width=\textwidth]{other}
	\caption{Other.}
	\end{subfigure}

	%\scalebox{0.5}{\input{figures/frame_transforms.pdf_tex}}
	\caption{Types.\label{fig:types}}
\end{figure}
%----------------------------------------------------------------------------



